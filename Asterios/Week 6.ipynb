{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Lightning into Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> *Exercises*: A few questions about machine learning.\n",
    "\n",
    "> * What do we mean by a 'feature' in a machine learning model?\n",
    "> * What is the main problem with overfitting?\n",
    "> * Explain the connection between the bias-variance trade-off and overfitting/underfitting.\n",
    "> * The `Luke is for leukemia` on page 145 in the reading is a great example of why accuracy is not a good measure in very unbalanced problems. You know about the incidents dataset we've been working with. Try to come up with a similar example based on the data we've been working with today."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Features are whatever inputs we provide to our model.\n",
    "\n",
    "* The main problem with overfitting is that our model, when we want to do \"predictive modelling\", learns on specific patterns learned by our training set. That includes that \"noise\" (no value information) we provided on the training, as well as insignificant patterns that are present on the training set but not on any real world examples that we are gonna test our predictive models on (it could involve learning to identify specific inputs rather than whatever factors are actually predictive for the desired output).\n",
    "\n",
    "* Overfitting comes with high variance and underfitting comes with high bias. If your model has high bias (which means it performs poorly even on your training data) then one thing to try is adding more features. If your model has high variance, then you can similarly remove features. But another solution is to obtain more data (if you can).\n",
    "\n",
    "* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Scikit-Learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> *Exercise*: Did you read the text?\n",
    ">\n",
    "> * Describe in your own words how data is organized in `sklearn` (how does a *dataset* work according to the tutorial)?\n",
    "> * What is the dimensionality of the `.data` part of a dataset and what is the size of each dimension?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* A dataset is a dictionary-like object that holds all the data and some metadata about the data. This data is stored in the .data member, which is a n_samples, n_features array. In the case of supervised problem, one or more response variables are stored in the .target member.\n",
    "\n",
    "* A .data part is a matrix whose number of rows is equal to the number of observations and the number of columns is equal to the number of features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> *Exercise*: Did you do the work?\n",
    ">\n",
    "> * Describe in your own words the dataset used in the tutorial. \n",
    "> * Investigate further: what kind of folder/file structure does the `sklearn.datasets.load_files` function expect?\n",
    "> * What is the \"bag-of-words\" representation of text? How does this strategy turn text into data of the kind described above?\n",
    "> * (Don't worry too much about tokenization and TF-IDF for now, but do check out those part if you want to use real text analysis later)\n",
    "> * Once you've built the classifier, play around with it a bit. Describe the content of the `predicted` variable.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* In the tutorial the dataset used is the [Iris flower data set](https://en.wikipedia.org/wiki/Iris_flower_data_set), [Handwritten Digits](https://archive.ics.uci.edu/ml/datasets/Pen-Based+Recognition+of+Handwritten+Digits) for classificaiton and for regression the [Diabetes dataset with its 10 features and target variable of 442 patients](https://www4.stat.ncsu.edu/~boos/var.select/diabetes.html)\n",
    "\n",
    "* It requires the path to the folder or the texts files.\n",
    "\n",
    "* In the bag-of-words representation we assign a fixed integer id to each word occurring in any document of the training set (for instance by building a dictionary from words to integer indices). For each document #i, count the number of occurrences of each word w and store it in X[i, j] as the value of feature #j where j is the index of word w in the dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
