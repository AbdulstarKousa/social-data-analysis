{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  ---\n",
    "\n",
    "  \n",
    " # <span style=\"color:MediumSlateBlue     \">Final Project | Explainer Notebook.</span>\n",
    "\n",
    " ## <span style=\"color:MediumSlateBlue     \">Part 01 - Data Preprocessing.</span>\n",
    "\n",
    "\n",
    "<span style=\"color:MediumSlateBlue     \">**02806 Social data analysis and visualization**</span>\n",
    "\n",
    "<span style=\"color:MediumSlateBlue     \">**May 2021**</span>\n",
    "\n",
    "<span style=\"color:MediumSlateBlue     \"> **Data-sets Reference: Motor-Vihecle-Collisions<sup>[link](https://data.cityofnewyork.us/Public-Safety/Motor-Vehicle-Collisions-Crashes/h9gi-nx95)</sup>, Weather-Data<sup>[link](https://www.ncdc.noaa.gov/cdo-web/search)</sup>, Speed-Limit-Data<sup>[link](https://data.cityofnewyork.us/Transportation/VZV_Speed-Limits/7n5j-865y)**</sup></span>\n",
    "\n",
    "  ---\n",
    "  \n",
    "\n",
    "  \n",
    "<span style=\"color:Orange\">**Please note!**</span> If you are using Jupyter to display this \".ipynb\" file You might need to make it *Trusted* in order to let Jupyter render the plots.\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "# <span style=\"color:MediumSlateBlue\">Import needed libraries:</span>\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" IPython \"\"\"\n",
    "from IPython.display import display\n",
    "from IPython import get_ipython\n",
    "get_ipython().run_line_magic('matplotlib', 'inline')\n",
    "\n",
    "\"\"\" Data Handeling \"\"\"\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import calendar\n",
    "import os \n",
    "from scipy import stats\n",
    "\n",
    "\"\"\" for warnings \"\"\"\n",
    "import warnings \n",
    "warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "# <span style=\"color:MediumSlateBlue\">Load data:</span>\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Path \"\"\"\n",
    "fileName = 'Motor_Vehicle_Collisions.csv'\n",
    "filePath = os.path.abspath(os.path.join(os.getcwd(), fileName))\n",
    "\n",
    "\"\"\" Load \"\"\"\n",
    "Data =  pd.read_csv(filePath);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "# <span style=\"color:MediumSlateBlue\">Getting to know the Dataset:</span>\n",
    "\n",
    "---\n",
    "\n",
    "Let's start in getting to know the dataset. In this section we introduce a function to track the **reduction in data** when doing the preparation and cleaning. \n",
    "\n",
    "The reader gets familiar with the whole process starting from **vewing the data**, the **column types** and the different ways of tracking the number of **missing values**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Define a function to track Reduction in data when doing the data preparation and cleaning later on \"\"\"\n",
    "\n",
    "Reduction = {}\n",
    "Reduction_Percentage = {}\n",
    "N = Data.shape[0]\n",
    "def reduc(step):\n",
    "    global N \n",
    "    global Reduction\n",
    "    global Reduction_Percentage\n",
    "    N_before = N\n",
    "    N_after = Data.shape[0]\n",
    "    Reduction[step] = N_after\n",
    "    Reduction_Percentage[step] = (N_before - N_after) / N_before\n",
    "    print(f'Number of observation: {N_after}  (--{N_before-N_after})')\n",
    "    print(f'Reduction: {N_before - N_after}  ({(N_before - N_after) / N_before} %)')\n",
    "    N = Data.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of observation: 1770881  (--0)\n",
      "Reduction: 0  (0.0 %)\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Initilize Reduction in data \"\"\"\n",
    "reduc('Init')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRASH DATE</th>\n",
       "      <th>CRASH TIME</th>\n",
       "      <th>BOROUGH</th>\n",
       "      <th>ZIP CODE</th>\n",
       "      <th>LATITUDE</th>\n",
       "      <th>LONGITUDE</th>\n",
       "      <th>LOCATION</th>\n",
       "      <th>ON STREET NAME</th>\n",
       "      <th>CROSS STREET NAME</th>\n",
       "      <th>OFF STREET NAME</th>\n",
       "      <th>...</th>\n",
       "      <th>CONTRIBUTING FACTOR VEHICLE 2</th>\n",
       "      <th>CONTRIBUTING FACTOR VEHICLE 3</th>\n",
       "      <th>CONTRIBUTING FACTOR VEHICLE 4</th>\n",
       "      <th>CONTRIBUTING FACTOR VEHICLE 5</th>\n",
       "      <th>COLLISION_ID</th>\n",
       "      <th>VEHICLE TYPE CODE 1</th>\n",
       "      <th>VEHICLE TYPE CODE 2</th>\n",
       "      <th>VEHICLE TYPE CODE 3</th>\n",
       "      <th>VEHICLE TYPE CODE 4</th>\n",
       "      <th>VEHICLE TYPE CODE 5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>07/24/2019</td>\n",
       "      <td>9:30</td>\n",
       "      <td>BRONX</td>\n",
       "      <td>10467</td>\n",
       "      <td>40.875328</td>\n",
       "      <td>-73.869225</td>\n",
       "      <td>(40.875328, -73.869225)</td>\n",
       "      <td>MAGENTA STREET</td>\n",
       "      <td>BARKER AVENUE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>Unspecified</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4195590</td>\n",
       "      <td>Station Wagon/Sport Utility Vehicle</td>\n",
       "      <td>Sedan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>08/13/2019</td>\n",
       "      <td>19:15</td>\n",
       "      <td>BRONX</td>\n",
       "      <td>10461</td>\n",
       "      <td>40.846370</td>\n",
       "      <td>-73.833140</td>\n",
       "      <td>(40.84637, -73.83314)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2937      WESTCHESTER AVENUE</td>\n",
       "      <td>...</td>\n",
       "      <td>Unspecified</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4189304</td>\n",
       "      <td>Station Wagon/Sport Utility Vehicle</td>\n",
       "      <td>Station Wagon/Sport Utility Vehicle</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>08/02/2019</td>\n",
       "      <td>23:15</td>\n",
       "      <td>MANHATTAN</td>\n",
       "      <td>10016</td>\n",
       "      <td>40.749100</td>\n",
       "      <td>-73.984085</td>\n",
       "      <td>(40.7491, -73.984085)</td>\n",
       "      <td>5 AVENUE</td>\n",
       "      <td>EAST 35 STREET</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>Unspecified</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4182289</td>\n",
       "      <td>Sedan</td>\n",
       "      <td>Station Wagon/Sport Utility Vehicle</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>08/16/2019</td>\n",
       "      <td>21:30</td>\n",
       "      <td>QUEENS</td>\n",
       "      <td>11358</td>\n",
       "      <td>40.755566</td>\n",
       "      <td>-73.791720</td>\n",
       "      <td>(40.755566, -73.79172)</td>\n",
       "      <td>45 AVENUE</td>\n",
       "      <td>189 STREET</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>Unspecified</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4189758</td>\n",
       "      <td>Station Wagon/Sport Utility Vehicle</td>\n",
       "      <td>Station Wagon/Sport Utility Vehicle</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>07/24/2019</td>\n",
       "      <td>18:05</td>\n",
       "      <td>MANHATTAN</td>\n",
       "      <td>10025</td>\n",
       "      <td>40.794520</td>\n",
       "      <td>-73.970020</td>\n",
       "      <td>(40.79452, -73.97002)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>755       AMSTERDAM AVENUE</td>\n",
       "      <td>...</td>\n",
       "      <td>Unspecified</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4176577</td>\n",
       "      <td>Sedan</td>\n",
       "      <td>Sedan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   CRASH DATE CRASH TIME    BOROUGH ZIP CODE   LATITUDE  LONGITUDE  \\\n",
       "0  07/24/2019       9:30      BRONX    10467  40.875328 -73.869225   \n",
       "1  08/13/2019      19:15      BRONX    10461  40.846370 -73.833140   \n",
       "2  08/02/2019      23:15  MANHATTAN    10016  40.749100 -73.984085   \n",
       "3  08/16/2019      21:30     QUEENS    11358  40.755566 -73.791720   \n",
       "4  07/24/2019      18:05  MANHATTAN    10025  40.794520 -73.970020   \n",
       "\n",
       "                  LOCATION                    ON STREET NAME  \\\n",
       "0  (40.875328, -73.869225)  MAGENTA STREET                     \n",
       "1    (40.84637, -73.83314)                               NaN   \n",
       "2    (40.7491, -73.984085)  5 AVENUE                           \n",
       "3   (40.755566, -73.79172)  45 AVENUE                          \n",
       "4    (40.79452, -73.97002)                               NaN   \n",
       "\n",
       "  CROSS STREET NAME                           OFF STREET NAME  ...  \\\n",
       "0     BARKER AVENUE                                       NaN  ...   \n",
       "1               NaN  2937      WESTCHESTER AVENUE              ...   \n",
       "2    EAST 35 STREET                                       NaN  ...   \n",
       "3        189 STREET                                       NaN  ...   \n",
       "4               NaN  755       AMSTERDAM AVENUE                ...   \n",
       "\n",
       "   CONTRIBUTING FACTOR VEHICLE 2  CONTRIBUTING FACTOR VEHICLE 3  \\\n",
       "0                    Unspecified                            NaN   \n",
       "1                    Unspecified                            NaN   \n",
       "2                    Unspecified                            NaN   \n",
       "3                    Unspecified                            NaN   \n",
       "4                    Unspecified                            NaN   \n",
       "\n",
       "   CONTRIBUTING FACTOR VEHICLE 4  CONTRIBUTING FACTOR VEHICLE 5  COLLISION_ID  \\\n",
       "0                            NaN                            NaN       4195590   \n",
       "1                            NaN                            NaN       4189304   \n",
       "2                            NaN                            NaN       4182289   \n",
       "3                            NaN                            NaN       4189758   \n",
       "4                            NaN                            NaN       4176577   \n",
       "\n",
       "                   VEHICLE TYPE CODE 1                  VEHICLE TYPE CODE 2  \\\n",
       "0  Station Wagon/Sport Utility Vehicle                                Sedan   \n",
       "1  Station Wagon/Sport Utility Vehicle  Station Wagon/Sport Utility Vehicle   \n",
       "2                                Sedan  Station Wagon/Sport Utility Vehicle   \n",
       "3  Station Wagon/Sport Utility Vehicle  Station Wagon/Sport Utility Vehicle   \n",
       "4                                Sedan                                Sedan   \n",
       "\n",
       "   VEHICLE TYPE CODE 3 VEHICLE TYPE CODE 4 VEHICLE TYPE CODE 5  \n",
       "0                  NaN                 NaN                 NaN  \n",
       "1                  NaN                 NaN                 NaN  \n",
       "2                  NaN                 NaN                 NaN  \n",
       "3                  NaN                 NaN                 NaN  \n",
       "4                  NaN                 NaN                 NaN  \n",
       "\n",
       "[5 rows x 29 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" Overview \"\"\"\n",
    "Data.head(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1770881, 29)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" Data shape \"\"\"\n",
    "Data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['CRASH DATE', 'CRASH TIME', 'BOROUGH', 'ZIP CODE', 'LATITUDE',\n",
       "       'LONGITUDE', 'LOCATION', 'ON STREET NAME', 'CROSS STREET NAME',\n",
       "       'OFF STREET NAME', 'NUMBER OF PERSONS INJURED',\n",
       "       'NUMBER OF PERSONS KILLED', 'NUMBER OF PEDESTRIANS INJURED',\n",
       "       'NUMBER OF PEDESTRIANS KILLED', 'NUMBER OF CYCLIST INJURED',\n",
       "       'NUMBER OF CYCLIST KILLED', 'NUMBER OF MOTORIST INJURED',\n",
       "       'NUMBER OF MOTORIST KILLED', 'CONTRIBUTING FACTOR VEHICLE 1',\n",
       "       'CONTRIBUTING FACTOR VEHICLE 2', 'CONTRIBUTING FACTOR VEHICLE 3',\n",
       "       'CONTRIBUTING FACTOR VEHICLE 4', 'CONTRIBUTING FACTOR VEHICLE 5',\n",
       "       'COLLISION_ID', 'VEHICLE TYPE CODE 1', 'VEHICLE TYPE CODE 2',\n",
       "       'VEHICLE TYPE CODE 3', 'VEHICLE TYPE CODE 4', 'VEHICLE TYPE CODE 5'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" Columns' names \"\"\"\n",
    "Data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CRASH DATE                        object\n",
       "CRASH TIME                        object\n",
       "BOROUGH                           object\n",
       "ZIP CODE                          object\n",
       "LATITUDE                         float64\n",
       "LONGITUDE                        float64\n",
       "LOCATION                          object\n",
       "ON STREET NAME                    object\n",
       "CROSS STREET NAME                 object\n",
       "OFF STREET NAME                   object\n",
       "NUMBER OF PERSONS INJURED        float64\n",
       "NUMBER OF PERSONS KILLED         float64\n",
       "NUMBER OF PEDESTRIANS INJURED      int64\n",
       "NUMBER OF PEDESTRIANS KILLED       int64\n",
       "NUMBER OF CYCLIST INJURED          int64\n",
       "NUMBER OF CYCLIST KILLED           int64\n",
       "NUMBER OF MOTORIST INJURED         int64\n",
       "NUMBER OF MOTORIST KILLED          int64\n",
       "CONTRIBUTING FACTOR VEHICLE 1     object\n",
       "CONTRIBUTING FACTOR VEHICLE 2     object\n",
       "CONTRIBUTING FACTOR VEHICLE 3     object\n",
       "CONTRIBUTING FACTOR VEHICLE 4     object\n",
       "CONTRIBUTING FACTOR VEHICLE 5     object\n",
       "COLLISION_ID                       int64\n",
       "VEHICLE TYPE CODE 1               object\n",
       "VEHICLE TYPE CODE 2               object\n",
       "VEHICLE TYPE CODE 3               object\n",
       "VEHICLE TYPE CODE 4               object\n",
       "VEHICLE TYPE CODE 5               object\n",
       "dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" Columns types \"\"\"\n",
    "Data.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we see the **NaN values per column**. The NYC collision dataset that we use, **log the 3rd, 4rd and 5th factors in a collision by loging the vehicle types and the accordingly contributing factors** but very reraly the is more than a second contributing factor.\n",
    "\n",
    "We realise that there are many missing values there because the collisions normally is limited between 2 factors, the cars in the car crash. \n",
    "\n",
    "These values will be handled later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('VEHICLE TYPE CODE 5', 1764339),\n",
       " ('CONTRIBUTING FACTOR VEHICLE 5', 1764162),\n",
       " ('VEHICLE TYPE CODE 4', 1745952),\n",
       " ('CONTRIBUTING FACTOR VEHICLE 4', 1745232),\n",
       " ('VEHICLE TYPE CODE 3', 1655176),\n",
       " ('CONTRIBUTING FACTOR VEHICLE 3', 1651915),\n",
       " ('OFF STREET NAME', 1505835),\n",
       " ('CROSS STREET NAME', 624333),\n",
       " ('ZIP CODE', 544616),\n",
       " ('BOROUGH', 544404),\n",
       " ('ON STREET NAME', 357414),\n",
       " ('VEHICLE TYPE CODE 2', 294318),\n",
       " ('CONTRIBUTING FACTOR VEHICLE 2', 251365),\n",
       " ('LATITUDE', 209460),\n",
       " ('LONGITUDE', 209460),\n",
       " ('LOCATION', 209460),\n",
       " ('VEHICLE TYPE CODE 1', 9368),\n",
       " ('CONTRIBUTING FACTOR VEHICLE 1', 4992),\n",
       " ('NUMBER OF PERSONS KILLED', 31),\n",
       " ('NUMBER OF PERSONS INJURED', 18),\n",
       " ('CRASH DATE', 0),\n",
       " ('CRASH TIME', 0),\n",
       " ('NUMBER OF PEDESTRIANS INJURED', 0),\n",
       " ('NUMBER OF PEDESTRIANS KILLED', 0),\n",
       " ('NUMBER OF CYCLIST INJURED', 0),\n",
       " ('NUMBER OF CYCLIST KILLED', 0),\n",
       " ('NUMBER OF MOTORIST INJURED', 0),\n",
       " ('NUMBER OF MOTORIST KILLED', 0),\n",
       " ('COLLISION_ID', 0)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" Count columns' NaN values in desending order \"\"\"\n",
    "sorted(list(zip(Data.columns,Data.isna().sum(axis=0).values)) , key= lambda row: row[1], reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are the attributes with data filled. `CRASH DATE` and `CRASH TIME` are the the first thing police officers write, as well as the **number of people killed or injured**. We see some missing values on generaly some `STREET NAMEs`, `BOROUGHs`, as well as the 3rd, 4th and 5th factors are rarely added for the aforementioned factors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('CRASH DATE', 1770881),\n",
       " ('CRASH TIME', 1770881),\n",
       " ('NUMBER OF PEDESTRIANS INJURED', 1770881),\n",
       " ('NUMBER OF PEDESTRIANS KILLED', 1770881),\n",
       " ('NUMBER OF CYCLIST INJURED', 1770881),\n",
       " ('NUMBER OF CYCLIST KILLED', 1770881),\n",
       " ('NUMBER OF MOTORIST INJURED', 1770881),\n",
       " ('NUMBER OF MOTORIST KILLED', 1770881),\n",
       " ('COLLISION_ID', 1770881),\n",
       " ('NUMBER OF PERSONS INJURED', 1770863),\n",
       " ('NUMBER OF PERSONS KILLED', 1770850),\n",
       " ('CONTRIBUTING FACTOR VEHICLE 1', 1765889),\n",
       " ('VEHICLE TYPE CODE 1', 1761513),\n",
       " ('LATITUDE', 1561421),\n",
       " ('LONGITUDE', 1561421),\n",
       " ('LOCATION', 1561421),\n",
       " ('CONTRIBUTING FACTOR VEHICLE 2', 1519516),\n",
       " ('VEHICLE TYPE CODE 2', 1476563),\n",
       " ('ON STREET NAME', 1413467),\n",
       " ('BOROUGH', 1226477),\n",
       " ('ZIP CODE', 1226265),\n",
       " ('CROSS STREET NAME', 1146548),\n",
       " ('OFF STREET NAME', 265046),\n",
       " ('CONTRIBUTING FACTOR VEHICLE 3', 118966),\n",
       " ('VEHICLE TYPE CODE 3', 115705),\n",
       " ('CONTRIBUTING FACTOR VEHICLE 4', 25649),\n",
       " ('VEHICLE TYPE CODE 4', 24929),\n",
       " ('CONTRIBUTING FACTOR VEHICLE 5', 6719),\n",
       " ('VEHICLE TYPE CODE 5', 6542)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" Count columns' Non-NaN values in desending order \"\"\"\n",
    "sorted(list(zip(Data.count().keys(),Data.count().values)), key= lambda row: row[1], reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we spot some **0's** in the `LATITUDE` and `LONGITUDE` that doesn't make sense. These values will be handled later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CRASH DATE                             0\n",
       "CRASH TIME                             0\n",
       "BOROUGH                                0\n",
       "ZIP CODE                               0\n",
       "LATITUDE                            1766\n",
       "LONGITUDE                           1766\n",
       "LOCATION                               0\n",
       "ON STREET NAME                         0\n",
       "CROSS STREET NAME                      0\n",
       "OFF STREET NAME                        0\n",
       "NUMBER OF PERSONS INJURED        1414110\n",
       "NUMBER OF PERSONS KILLED         1767530\n",
       "NUMBER OF PEDESTRIANS INJURED    1682257\n",
       "NUMBER OF PEDESTRIANS KILLED     1768543\n",
       "NUMBER OF CYCLIST INJURED        1729563\n",
       "NUMBER OF CYCLIST KILLED         1769535\n",
       "NUMBER OF MOTORIST INJURED       1540017\n",
       "NUMBER OF MOTORIST KILLED        1768883\n",
       "CONTRIBUTING FACTOR VEHICLE 1          0\n",
       "CONTRIBUTING FACTOR VEHICLE 2          0\n",
       "CONTRIBUTING FACTOR VEHICLE 3          0\n",
       "CONTRIBUTING FACTOR VEHICLE 4          0\n",
       "CONTRIBUTING FACTOR VEHICLE 5          0\n",
       "COLLISION_ID                           0\n",
       "VEHICLE TYPE CODE 1                    0\n",
       "VEHICLE TYPE CODE 2                    0\n",
       "VEHICLE TYPE CODE 3                    0\n",
       "VEHICLE TYPE CODE 4                    0\n",
       "VEHICLE TYPE CODE 5                    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" Count columns' zeros values \"\"\"\n",
    "(Data == 0).sum(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some times **NaN values can come in form of empty strings**. We check this posibility in case we spot some extra missing values. There are not any."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CRASH DATE                       0\n",
       "CRASH TIME                       0\n",
       "BOROUGH                          0\n",
       "ZIP CODE                         0\n",
       "LATITUDE                         0\n",
       "LONGITUDE                        0\n",
       "LOCATION                         0\n",
       "ON STREET NAME                   0\n",
       "CROSS STREET NAME                0\n",
       "OFF STREET NAME                  0\n",
       "NUMBER OF PERSONS INJURED        0\n",
       "NUMBER OF PERSONS KILLED         0\n",
       "NUMBER OF PEDESTRIANS INJURED    0\n",
       "NUMBER OF PEDESTRIANS KILLED     0\n",
       "NUMBER OF CYCLIST INJURED        0\n",
       "NUMBER OF CYCLIST KILLED         0\n",
       "NUMBER OF MOTORIST INJURED       0\n",
       "NUMBER OF MOTORIST KILLED        0\n",
       "CONTRIBUTING FACTOR VEHICLE 1    0\n",
       "CONTRIBUTING FACTOR VEHICLE 2    0\n",
       "CONTRIBUTING FACTOR VEHICLE 3    0\n",
       "CONTRIBUTING FACTOR VEHICLE 4    0\n",
       "CONTRIBUTING FACTOR VEHICLE 5    0\n",
       "COLLISION_ID                     0\n",
       "VEHICLE TYPE CODE 1              0\n",
       "VEHICLE TYPE CODE 2              0\n",
       "VEHICLE TYPE CODE 3              0\n",
       "VEHICLE TYPE CODE 4              0\n",
       "VEHICLE TYPE CODE 5              0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" Count columns' empty strings \"\"\"\n",
    "(Data == '').sum(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \n",
    "---\n",
    "\n",
    "# <span style=\"color:MediumSlateBlue\">Data Cleaning:</span>\n",
    "\n",
    "---\n",
    "\n",
    "After this  initial check on the data, we are proceeding into the Data Cleaning.\n",
    "\n",
    "In this section we **finalize the New York City Collisions dataset from 2013-2020** in order to merge (in the next section) the **weather and speed limit features**. \n",
    "\n",
    "The steps in doing the clean consider **dropping unneeded features** with the accordingly justification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drop unneeded features:\n",
    "\n",
    "In this section we drop the `COLLISION_ID` since it's not informative. We drop the `LOCATION` since we have `LATITUDE` and `LONGITUTE`. We drop the `PEDESTRIANS`, `CYCLISTS`, `MOTORIST` features sience we have the `NUMBER OF PERSONS INJURED` and `NUMBER OF PERSONS KILLED` features and finally we drop the 3rd, 4th and 4th collision factors (as discussed above).\n",
    "\n",
    "Finally, we check for the ammount of data dropped overall. We see that we **drop only the 0,3% of the total data**. That is a very satisfying overall!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of observation: 1763247  (--6455)\n",
      "Reduction: 6455  (0.0036475067553746335 %)\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Drop 'COLLISION_ID' since it's not informative \"\"\"\n",
    "Data = Data.drop(columns=['COLLISION_ID'])\n",
    "\n",
    "\"\"\" Drop 'LOCATION' since we have 'LATITUDE', 'LONGITUDE' \"\"\"\n",
    "Data = Data.drop(columns=['LOCATION'])\n",
    "\n",
    "\"\"\" Drop 'CROSS STREET NAME' and 'OFF STREET NAME' since we have 'ON STREET NAME' \"\"\"\n",
    "Data = Data.drop(columns=['CROSS STREET NAME', 'OFF STREET NAME'])\n",
    "\n",
    "\"\"\" Drop PEDESTRIANS, CYCLIST and MOTORIST features since we have PERSONS features \"\"\"\n",
    "Data = Data.drop(columns = ['NUMBER OF PEDESTRIANS INJURED','NUMBER OF PEDESTRIANS KILLED', \n",
    "                            'NUMBER OF CYCLIST INJURED','NUMBER OF CYCLIST KILLED', \n",
    "                            'NUMBER OF MOTORIST INJURED','NUMBER OF MOTORIST KILLED'])\n",
    "\n",
    "\"\"\" Consider only Collisions with two vehicles involve and Drop other unrelated features \"\"\"\n",
    "Data = Data[\n",
    "        (Data['CONTRIBUTING FACTOR VEHICLE 3'].isna())|\n",
    "        (Data['CONTRIBUTING FACTOR VEHICLE 4'].isna())|\n",
    "        (Data['CONTRIBUTING FACTOR VEHICLE 5'].isna())|\n",
    "        (Data['VEHICLE TYPE CODE 3'].isna())|\n",
    "        (Data['VEHICLE TYPE CODE 4'].isna())|\n",
    "        (Data['VEHICLE TYPE CODE 5'].isna())]\n",
    "Data = Data.drop(columns=['CONTRIBUTING FACTOR VEHICLE 3','CONTRIBUTING FACTOR VEHICLE 4','CONTRIBUTING FACTOR VEHICLE 5','VEHICLE TYPE CODE 3', 'VEHICLE TYPE CODE 4', 'VEHICLE TYPE CODE 5'])\n",
    "\n",
    "\"\"\" Track Reduction in data \"\"\"\n",
    "reduc('MVC with only two vehicles involves')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \n",
    "---\n",
    "\n",
    "## <span style=\"color:MediumSlateBlue\">Missing Data:</span>\n",
    "\n",
    "---\n",
    "\n",
    "In this section we drop all the NaN values on the attributes `ON STREET NAME`, `LATITUDE`, `LONGITUDE`, `NUMBER OF PERSONS KILLED` and `NUMBER OF PERSONS INJURED`. We also drop the accidents that have `LATITUDE` and `LONGITUDE` == 0 because the location is not recorded. \n",
    "\n",
    "Finally, we see that we loose **29% of the data** but still the number of rows is more than sufiecient to have a represented dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of observation: 1250921  (--512326)\n",
      "Reduction: 512326  (0.290558271189459 %)\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Count columns' NaN values in desending order \"\"\"\n",
    "sorted(list(zip(Data.columns,Data.isna().sum(axis=0).values)) , key= lambda row: row[1], reverse=True)\n",
    "\n",
    "\"\"\" Count columns' zeros values \"\"\"\n",
    "(Data == 0).sum(axis=0)\n",
    "\n",
    "\"\"\" Count columns' empty strings \"\"\"\n",
    "(Data == '').sum(axis=0)\n",
    "\n",
    "\"\"\" Drop rows that has a messing value in one of important features \"\"\"\n",
    "Data = Data[\n",
    "    Data['ON STREET NAME'].notna()  & # important feature for adding speed limit data later on.\n",
    "    Data['LATITUDE' ].notna()       & # imporatnt feature for map plots \n",
    "    Data['LONGITUDE'].notna()       & # imporatnt feature for map plots\n",
    "    Data['NUMBER OF PERSONS INJURED'].notna()   & # imporatnt feature since one of the main features of intress\n",
    "    Data['NUMBER OF PERSONS KILLED'].notna()      # imporatnt feature since one of the main features of intress\n",
    "    ].copy()\n",
    "\n",
    "\"\"\" Drop rows with LATITUDE or LONGITUDE = 0 \"\"\"\n",
    "Data = Data[(Data['LATITUDE']!=0)|(Data['LONGITUDE']!=0)].copy()\n",
    "\n",
    "\"\"\" Track Reduction in data \"\"\"\n",
    "reduc('Drop missing values in important features')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \n",
    "---\n",
    "\n",
    "# <span style=\"color:MediumSlateBlue\">Features Prepration:</span>\n",
    "\n",
    "---\n",
    "In this section a **cleaning** of the categorical values, those are taking place on important factors like the `VEHICLE TYPES` and `CONTRIBUTING FACTORS` attributes. We also clean the `ZIP` column. The justification is described below.\n",
    "\n",
    "After that, we **create new features**. Most importantly the **`Response` attribute** that indicates if **victims of the collision got injured or died** (in the specific car accident) and indicating time reference features like the `Year` of the accident as well as the `Month`, `Day`, `Hour`, `Day of week` and `Minute`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Vehicle types:\n",
    "\n",
    "Our goal in the preperation of the vehicle types without introducing **any bias to the data**. That means the scope of this preparation doesn't involve handwritten merging subcategories other than **fixing typos and then select the types that matter for the 95% Frequency of the MCV Vehicle types.**\n",
    " \n",
    " The methodology is as following;\n",
    " \n",
    " 1. Select the Vehicle Types that has more than 50 motor vehicle collision (MVC) occurrences\n",
    " 2. Replace typos (with no subcategory merge in order to avoid bias)\n",
    " 3. Consider only the vehicle types that occur for the 95% of vehicle collisions (MVC)\n",
    " \n",
    "We see that the final **vehicles type 1** involve the following **(11) vehicles**: 'sport utility vehicle', 'sedan', 'passenger vehicle', 'taxi', 'pick-up truck', 'van', 'bus', 'unknown', 'other', 'box truck', 'small com veh(4 tires)'.\n",
    "\n",
    "We do the same for the **vehicle type 2** and we end up with the following vehicles: 'sport utility vehicle', 'unknown', 'sedan', 'passenger vehicle', 'taxi', 'bike', 'pick-up truck', 'van', 'bus', 'other', 'box truck'.\n",
    "\n",
    "---\n",
    "\n",
    "- What we realise is that **bikes are a popular type 2 vehicle collision type** but not a type 1‼  \n",
    "\n",
    "---\n",
    "\n",
    "At the end, we **combine the two vehicle types categorical values** and we end up with only **loosing 5% of the data**. This is very welcoming. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **Prepare Vehicle type 1:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Focus Vehicles Type 1 (95 % Frequent):\n",
      "['sport utility vehicle', 'sedan', 'passenger vehicle', 'taxi', 'pick-up truck', 'van', 'bus', 'unknown', 'other', 'box truck', 'small com veh(4 tires)']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Unify Vehicle type recording way \"\"\"\n",
    "Data['VEHICLE TYPE CODE 1'] = Data['VEHICLE TYPE CODE 1'].str.lower()\n",
    "Data['VEHICLE TYPE CODE 1'] = Data['VEHICLE TYPE CODE 1'].str.strip()\n",
    "\n",
    "\"\"\" Fixing recording issus of Vehicle types that has more than 50 MVC occurrences \"\"\"\n",
    "Frequent_MVC_Vehicles = (Data['VEHICLE TYPE CODE 1'].value_counts().keys()[Data['VEHICLE TYPE CODE 1'].value_counts().values > 50])\n",
    "\n",
    "Mapping = { # Based on Frequent_MVC_Vehicles values\n",
    "    np.nan: 'unknown',\n",
    "    'station wagon/sport utility vehicle': 'sport utility vehicle', \n",
    "    'sport utility / station wagon':'sport utility vehicle', \n",
    "    '4 dr sedan': 'sedan', \n",
    "    'ambul': 'ambulance',  \n",
    "    'school bus': 'school bus', \n",
    "    'e-sco': 'e-scooter', \n",
    "    'schoo': 'school bus', \n",
    "    'bicycle': 'bike'\n",
    "    }\n",
    "\n",
    "Data['VEHICLE TYPE CODE 1'] = Data['VEHICLE TYPE CODE 1'].replace(Mapping)\n",
    "\n",
    "\"\"\" Consider only 95 % Frequent MVC Vehicle types \"\"\"\n",
    "VT1 = pd.DataFrame()\n",
    "VT1['VEHICLE TYPE CODE 1'] = Data['VEHICLE TYPE CODE 1'].value_counts(normalize=True).keys()\n",
    "VT1['Frequencies'] = Data['VEHICLE TYPE CODE 1'].value_counts(normalize=True).values\n",
    "\n",
    "threshold = 0\n",
    "for i in range(len(VT1['VEHICLE TYPE CODE 1'].unique())):\n",
    "    Sum = VT1['Frequencies'][0:i+1].sum()\n",
    "    if Sum > 0.95:\n",
    "         threshold = i + 1\n",
    "        #  print(\"Threshold that covers 95% of \" + \"VEHICLE TYPEs\".lower() +  \" = \" + f\"{threshold}\")\n",
    "         break \n",
    "Focus_Vehicles_Type_1 = list(VT1['VEHICLE TYPE CODE 1'][0:threshold].values)\n",
    "print('Focus Vehicles Type 1 (95 % Frequent):')\n",
    "print(Focus_Vehicles_Type_1)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Prepare Vehicle type 2:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Focus Vehicles Type 2 (95 % Frequent):\n",
      "['sport utility vehicle' 'unknown' 'sedan' 'passenger vehicle' 'taxi'\n",
      " 'bike' 'pick-up truck' 'van' 'bus' 'other' 'box truck']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Unify Vehicle type recording way \"\"\"\n",
    "Data['VEHICLE TYPE CODE 2'] = Data['VEHICLE TYPE CODE 2'].str.lower()\n",
    "Data['VEHICLE TYPE CODE 2'] = Data['VEHICLE TYPE CODE 2'].str.strip()\n",
    "\n",
    "\"\"\" Fixing recording issus of Vehicle types that has more than 50 MVC occurrences \"\"\"\n",
    "Frequent_MVC_Vehicles = (Data['VEHICLE TYPE CODE 2'].value_counts().keys()[Data['VEHICLE TYPE CODE 2'].value_counts().values > 50])\n",
    "\n",
    "Mapping = { # Based on Frequent_MVC_Vehicles values\n",
    "    np.nan: 'unknown',\n",
    "    'unkno': 'unknown',\n",
    "    'unk': 'unknown',\n",
    "    'station wagon/sport utility vehicle': 'sport utility vehicle', \n",
    "    'sport utility / station wagon':'sport utility vehicle', \n",
    "    '4 dr sedan': 'sedan', \n",
    "    'ambul': 'ambulance',  \n",
    "    'school bus': 'school bus', \n",
    "    'e-sco': 'e-scooter', \n",
    "    'schoo': 'school bus', \n",
    "    'bicycle': 'bike', \n",
    "    }\n",
    "\n",
    "Data['VEHICLE TYPE CODE 2'] = Data['VEHICLE TYPE CODE 2'].replace(Mapping)\n",
    "\n",
    "\"\"\" Consider only 95 % Frequent MVC Vehicle types \"\"\"\n",
    "VT2 = pd.DataFrame()\n",
    "VT2['VEHICLE TYPE CODE 2'] = Data['VEHICLE TYPE CODE 2'].value_counts(normalize=True).keys()\n",
    "VT2['Frequencies'] = Data['VEHICLE TYPE CODE 2'].value_counts(normalize=True).values\n",
    "\n",
    "threshold = 0\n",
    "for i in range(len(VT2['VEHICLE TYPE CODE 2'].unique())):\n",
    "    Sum = VT2['Frequencies'][0:i+1].sum()\n",
    "    if Sum > 0.95:\n",
    "         threshold = i + 1\n",
    "        #  print(\"Threshold that cover 95% of \" + \"VEHICLE TYPEs\".lower() +  \" = \" + f\"{threshold}\")\n",
    "         break \n",
    "Focus_Vehicles_Type_2 = VT2['VEHICLE TYPE CODE 2'][0:threshold].values\n",
    "\n",
    "print('Focus Vehicles Type 2 (95 % Frequent):')\n",
    "print(Focus_Vehicles_Type_2)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Slice Focus Vehicle Types (covers more than 95 % of MVC occurrences)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Focus Vehicles ( More than 95 % Frequent):\n",
      "['other', 'passenger vehicle', 'bike', 'unknown', 'pick-up truck', 'box truck', 'small com veh(4 tires)', 'sedan', 'van', 'sport utility vehicle', 'bus', 'taxi']\n",
      "\n",
      "Number of observation: 1150870  (--100051)\n",
      "Reduction: 100051  (0.07998186935865655 %)\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Slice \"\"\"\n",
    "Focus_Vehicle_Types = list(set(list(Focus_Vehicles_Type_1) + list(Focus_Vehicles_Type_2))) \n",
    "Data = Data[Data['VEHICLE TYPE CODE 1'].isin((Focus_Vehicle_Types)) & (Data['VEHICLE TYPE CODE 2'].isin(Focus_Vehicle_Types))].copy()\n",
    "print('Focus Vehicles ( More than 95 % Frequent):')\n",
    "print(Focus_Vehicle_Types)\n",
    "print()\n",
    "\n",
    "\"\"\" Track Reduction in data \"\"\"\n",
    "reduc('Slice Focus Vehicle Types')\n",
    "\n",
    "\"\"\" free memory \"\"\"\n",
    "del(VT1,VT2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Contributing Factors:\n",
    "\n",
    "Our goal in the contributing factors remain the same as the vehicle types. The goal is to **avoid any bias to the data**. That means the scope of this preparation doesn't involve handwritten merging subcategories other than **fixing typos and then select the types that matter for the 95% Frequency of the MCV Vehicle contributing factors.**\n",
    " \n",
    " The methodology is as following;\n",
    " \n",
    " 1. Select the contributing factors that has more than 50 MVC occurrences\n",
    " 2. Replace typos (with no subcategory merge in order to avoid bias)\n",
    " 3. Consider only the vehicle types that occur for the 95% of vehicle collisions (MVC)\n",
    " \n",
    "We see that the final **contributing factors type 1** involve the following **(20) factors**: 'unspecified', 'driver inattention/distraction', 'failure to yield right-of-way', 'following too closely', 'passing or lane usage improper', 'backing unsafely', 'other vehicular', 'turning improperly', 'fatigued/drowsy', 'unsafe lane changing', 'traffic control disregarded', 'driver inexperience', 'lost consciousness', 'reaction to uninvolved vehicle', 'unsafe speed', 'pavement slippery', 'prescription medication', 'alcohol involvement', 'physical disability', 'outside car distraction'.\n",
    "\n",
    "We do the same for the **vehicle type 2** and we end up with the following **(7) factors**: 'unspecified', 'unknown', 'driver inattention/distraction', 'other vehicular', 'passing or lane usage improper', 'failure to yield right-of-way', 'following too closely'\n",
    "\n",
    "---\n",
    "\n",
    "- What we realise is the **vehicle type 1 mostly has most the variation in the collision**. \n",
    "\n",
    "---\n",
    "\n",
    "At the end, we **combine the two vehicle types contributing factors** and we end up with only **loosing 4.7% of the data**. This is very welcoming. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Prepare Contributing Factor 1:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Focus Factors 1 (95 % Frequent):\n",
      "['unspecified', 'driver inattention/distraction', 'failure to yield right-of-way', 'following too closely', 'passing or lane usage improper', 'backing unsafely', 'other vehicular', 'turning improperly', 'fatigued/drowsy', 'unsafe lane changing', 'traffic control disregarded', 'driver inexperience', 'lost consciousness', 'reaction to uninvolved vehicle', 'unsafe speed', 'pavement slippery', 'prescription medication', 'alcohol involvement', 'physical disability', 'outside car distraction']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Unify Contributing Factor string \"\"\"\n",
    "Data['CONTRIBUTING FACTOR VEHICLE 1'] = Data['CONTRIBUTING FACTOR VEHICLE 1'].str.lower()\n",
    "Data['CONTRIBUTING FACTOR VEHICLE 1'] = Data['CONTRIBUTING FACTOR VEHICLE 1'].str.strip()\n",
    "\n",
    "\"\"\" Fixing recording issus of Contributing Factor that has more than 50 MVC occurrences \"\"\"\n",
    "Frequent_MVC_Factors = (Data['CONTRIBUTING FACTOR VEHICLE 1'].value_counts().keys()[Data['CONTRIBUTING FACTOR VEHICLE 1'].value_counts().values > 50])\n",
    "\n",
    "Mapping = { # Based on Frequent_MVC_Factors\n",
    "    np.nan: 'unknown',\n",
    "    'illnes':'illness', \n",
    "    'reaction to other uninvolved vehicle':'reaction to uninvolved vehicle',\n",
    "    'passing too closely': 'passing or lane usage improper',\n",
    "    }\n",
    "\n",
    "Data['CONTRIBUTING FACTOR VEHICLE 1'] = Data['CONTRIBUTING FACTOR VEHICLE 1'].replace(Mapping)\n",
    "\n",
    "\"\"\" Consider only 95 % Frequent MVC Contributing Factors \"\"\"\n",
    "CF1 = pd.DataFrame()\n",
    "CF1['CONTRIBUTING FACTOR VEHICLE 1'] = Data['CONTRIBUTING FACTOR VEHICLE 1'].value_counts(normalize=True).keys()\n",
    "CF1['Frequencies'] = Data['CONTRIBUTING FACTOR VEHICLE 1'].value_counts(normalize=True).values\n",
    "\n",
    "threshold = 0\n",
    "for i in range(len(CF1['CONTRIBUTING FACTOR VEHICLE 1'].unique())):\n",
    "    Sum = CF1['Frequencies'][0:i+1].sum()\n",
    "    if Sum > 0.95:\n",
    "         threshold = i + 1\n",
    "        #  print(\"Threshold that covers 95% of \" + \"CONTRIBUTING FACTORs\".lower() +  \" = \" + f\"{threshold}\")\n",
    "         break \n",
    "Focus_Factors_Type_1 = list(CF1['CONTRIBUTING FACTOR VEHICLE 1'][0:threshold].values)\n",
    "print('Focus Factors 1 (95 % Frequent):')\n",
    "print(Focus_Factors_Type_1)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Prepare Contributing Factor 2:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Focus Factors 2 (95 % Frequent):\n",
      "['unspecified', 'unknown', 'driver inattention/distraction', 'other vehicular', 'passing or lane usage improper', 'failure to yield right-of-way', 'following too closely']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Unify Contributing Factor string \"\"\"\n",
    "Data['CONTRIBUTING FACTOR VEHICLE 2'] = Data['CONTRIBUTING FACTOR VEHICLE 2'].str.lower()\n",
    "Data['CONTRIBUTING FACTOR VEHICLE 2'] = Data['CONTRIBUTING FACTOR VEHICLE 2'].str.strip()\n",
    "\n",
    "\"\"\" Fixing recording issus of Contributing Factor that has more than 50 MVC occurrences \"\"\"\n",
    "Frequent_MVC_Factors = (Data['CONTRIBUTING FACTOR VEHICLE 2'].value_counts().keys()[Data['CONTRIBUTING FACTOR VEHICLE 2'].value_counts().values > 50])\n",
    "\n",
    "Mapping = { # Based on Frequent_MVC_Factors\n",
    "    np.nan: 'unknown',\n",
    "    'illnes':'illness', \n",
    "    'reaction to other uninvolved vehicle':'reaction to uninvolved vehicle',\n",
    "    'passing too closely': 'passing or lane usage improper',\n",
    "    }\n",
    "\n",
    "Data['CONTRIBUTING FACTOR VEHICLE 2'] = Data['CONTRIBUTING FACTOR VEHICLE 2'].replace(Mapping)\n",
    "\n",
    "\"\"\" Consider only 95 % Frequent MVC Contributing Factors \"\"\"\n",
    "CF2 = pd.DataFrame()\n",
    "CF2['CONTRIBUTING FACTOR VEHICLE 2'] = Data['CONTRIBUTING FACTOR VEHICLE 2'].value_counts(normalize=True).keys()\n",
    "CF2['Frequencies'] = Data['CONTRIBUTING FACTOR VEHICLE 2'].value_counts(normalize=True).values\n",
    "\n",
    "threshold = 0\n",
    "for i in range(len(CF2['CONTRIBUTING FACTOR VEHICLE 2'].unique())):\n",
    "    Sum = CF2['Frequencies'][0:i+1].sum()\n",
    "    if Sum > 0.95:\n",
    "         threshold = i + 1\n",
    "        #  print(\"Threshold that covers 95% of \" + \"CONTRIBUTING FACTORs\".lower() +  \" = \" + f\"{threshold}\")\n",
    "         break \n",
    "Focus_Factors_Type_2 = list(CF2['CONTRIBUTING FACTOR VEHICLE 2'][0:threshold].values)\n",
    "print('Focus Factors 2 (95 % Frequent):')\n",
    "print(Focus_Factors_Type_2)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Slice Focus Factors Type (covers more than 95 % of MVC occurrences)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Focus Factors ( More than 95 % Frequent):\n",
      "['driver inattention/distraction', 'passing or lane usage improper', 'unsafe speed', 'other vehicular', 'fatigued/drowsy', 'turning improperly', 'pavement slippery', 'lost consciousness', 'alcohol involvement', 'physical disability', 'reaction to uninvolved vehicle', 'backing unsafely', 'traffic control disregarded', 'driver inexperience', 'unspecified', 'failure to yield right-of-way', 'following too closely', 'unknown', 'unsafe lane changing', 'outside car distraction', 'prescription medication']\n",
      "\n",
      "Number of observation: 1096004  (--54866)\n",
      "Reduction: 54866  (0.04767349917888206 %)\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Slice \"\"\"\n",
    "Focus_Factors_Types = list(set(list(Focus_Factors_Type_1) + list(Focus_Factors_Type_2))) \n",
    "Data = Data[Data['CONTRIBUTING FACTOR VEHICLE 1'].isin((Focus_Factors_Types)) & (Data['CONTRIBUTING FACTOR VEHICLE 2'].isin(Focus_Factors_Types))].copy()\n",
    "print('Focus Factors ( More than 95 % Frequent):')\n",
    "print(Focus_Factors_Types)\n",
    "print()\n",
    "\n",
    "\"\"\" Track Reduction in data \"\"\"\n",
    "reduc('Slice Focus Factors Types')\n",
    "\n",
    "\"\"\" free memory \"\"\"\n",
    "del(CF1,CF2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Zip Features:\n",
    "\n",
    "Here we replace unspecified string of `ZIP CODE` as NaN and the change the zip type to `float64`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Drop Unspecified Zip \"\"\"\n",
    "Data['ZIP CODE'].replace(to_replace='     ', value=np.nan, inplace=True)\n",
    "\n",
    "\"\"\" Change the Zip type to float64 \"\"\" \n",
    "Data['ZIP CODE'] = pd.to_numeric(Data['ZIP CODE']) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ##  Extract new features:\n",
    " \n",
    "Here we add two types of features:\n",
    " \n",
    "1. As discussed above we add the most important feature called `Response`. It shows as discussed in the beginning of the section **whether we have an injured or dead victim**.\n",
    "\n",
    "2. Other time related features like Year, Month, Day, Hour etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Add the 'Respone' feature, which is a binary future that says 0 if there is no injures or killed person and 1 other wise. \"\"\"\n",
    "Data['Response'] = Data[['NUMBER OF PERSONS INJURED','NUMBER OF PERSONS KILLED']].sum(axis=1)\n",
    "Data['Response'] = Data['Response'].apply(lambda y: 1 if y > 0 else 0)\n",
    "\n",
    "\"\"\" Add 'Year' feature \"\"\"\n",
    "Data['Year']    = pd.to_datetime(Data['CRASH DATE']).dt.year\n",
    "\n",
    "\"\"\" Add 'Month' feature \"\"\"\n",
    "Data['Month']    = pd.to_datetime(Data['CRASH DATE']).dt.month\n",
    "\n",
    "\"\"\" Add 'Day' feature \"\"\"\n",
    "Data['Day'] = pd.to_datetime(Data['CRASH DATE']).dt.day\n",
    "\n",
    "\"\"\" 'Day of week' feature \"\"\"\n",
    "Data['Day of week'] = pd.to_datetime(Data['CRASH DATE']).dt.day_name()\n",
    "\n",
    "\"\"\" Add 'Hour' feature \"\"\"\n",
    "Data['Hour'] = pd.to_datetime(Data['CRASH TIME']).dt.hour\n",
    "\n",
    "\"\"\" Add 'Minute' feature \"\"\"\n",
    "Data['Minute'] = pd.to_datetime(Data['CRASH TIME']).dt.minute"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drop uncompleted years:\n",
    "\n",
    "As a final thing we drop the year 2012 and 2021 due to the fact that they are **not completed**.\n",
    "\n",
    "We realise we see **we loose only 7.4%** of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of observation: 1014071  (--81933)\n",
      "Reduction: 81933  (0.07475611402878092 %)\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Drop rows from 2012 since they are not completed  \"\"\"\n",
    "Data = Data[Data['Year']!=2012]\n",
    "\n",
    "\"\"\" Drop rows from 2021 since they are not completed  \"\"\"\n",
    "Data = Data[Data['Year']!=2021]\n",
    "\n",
    "\"\"\" Track Reduction in data \"\"\"\n",
    "reduc('Drop uncompleted years')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \n",
    "---\n",
    "\n",
    "# <span style=\"color:MediumSlateBlue\">Adding new Datasets:</span>\n",
    "\n",
    "---\n",
    "\n",
    "In this section we merge **information about the street limits of NYC and weather**. Those will improve the forecasting of the Machine Learning part, hopefully!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding Speed_Limits Mode Data:\n",
    "\n",
    "For the Speed Limits we **select as limit the mode of the (mph) limit on avenues**. The processing is the following;\n",
    "\n",
    "1. Drop missing values on streets and limits\n",
    "2. Use `.lower()` and `.strip()` on both datasets (MVC - Street Speed Limits) to do the merging \n",
    "3. See the number of matched Steets\n",
    "4. Calculate the mode speed for the matched Streets\n",
    "5. Measure the data reduction\n",
    "\n",
    "We realise that we lose only **5.4% of the data**. Super nice!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging Speed Limits:\n",
      "    Number of Matched Streets = 5994\n",
      "    Number of Unmatched Streets = 1357\n",
      "Number of observation: 958371  (--55700)\n",
      "Reduction: 55700  (0.05492712048761872 %)\n"
     ]
    }
   ],
   "source": [
    "\"\"\" path \"\"\"\n",
    "fileName = 'dot_VZV_Speed_Limits_20210507.csv'\n",
    "filePath = os.path.abspath(os.path.join(os.getcwd(), fileName))\n",
    "\n",
    "\"\"\" load \"\"\"\n",
    "speed_limits =  pd.read_csv(filePath)\n",
    "\n",
    "\"\"\" Drop speed limits rows with missing values in important features \"\"\"\n",
    "speed_limits = speed_limits[\n",
    "        speed_limits['street'].notna()  &\n",
    "        speed_limits['postvz_sl'].notna()  \n",
    "    ].copy()\n",
    "\n",
    "\"\"\" Prepare street name features of both datasets for merging \"\"\"\n",
    "Data.loc[:,'ON STREET NAME'] = Data['ON STREET NAME'].str.lower()\n",
    "Data.loc[:,'ON STREET NAME'] = Data['ON STREET NAME'].str.strip()\n",
    "speed_limits.loc[:,'street'] = speed_limits['street'].str.lower()\n",
    "speed_limits.loc[:,'street'] = speed_limits['street'].str.strip()\n",
    "\n",
    "Matched_streets = Data['ON STREET NAME'][Data['ON STREET NAME'].isin(speed_limits['street'])].unique()\n",
    "print('Merging Speed Limits:') \n",
    "print(f\"    Number of Matched Streets = {len(Matched_streets)}\")\n",
    "print(f\"    Number of Unmatched Streets = {len(Data['ON STREET NAME'].unique()) - len(Matched_streets)}\")\n",
    "\n",
    "\"\"\" Calculate speed limits mode\"\"\"\n",
    "Street_Speed_Mode = {}\n",
    "streets = speed_limits['street'].unique()\n",
    "for street in streets:\n",
    "    Street_Values = speed_limits[speed_limits['street']==street]['postvz_sl']\n",
    "    Street_Mode = stats.mode(Street_Values)[0][0]\n",
    "    Street_Speed_Mode[street]= Street_Mode\n",
    "\n",
    "\"\"\" Add speed limits mode to Data \"\"\"\n",
    "Data = Data[Data['ON STREET NAME'].isin(streets)].copy()\n",
    "Data['SPEED LIMIT MODE'] = Data['ON STREET NAME'].apply(lambda street: Street_Speed_Mode[street])\n",
    "\n",
    "\"\"\" Track Reduction in data \"\"\"\n",
    "reduc('Adding Speed_Limits')\n",
    "\n",
    "\"\"\" Free memory \"\"\"\n",
    "del(speed_limits,Matched_streets, Street_Speed_Mode, streets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Adding weather data:\n",
    " \n",
    "To add weather data we prepare **date, wind, rain, snow, fog/vision\".\n",
    " \n",
    "- For the **rain** we take into account the `PRECIPITATION`.\n",
    "\n",
    "- For **snow fall** we take into account the `SNOW FALL` and `SNOW DEPTH`.\n",
    "\n",
    "- For  **fog/vision related** features we take into account a feature that tell whether there is an `FOG, SMOKE OR HAZE`.\n",
    "\n",
    "Lastly, we add temperature and wind speed for selfexplanatory reasons. We end up with the features;\n",
    "\n",
    "- `AVERAGE WIND SPEED`\n",
    "- `MAXIMUM TEMPERATURE`\n",
    "- `MINIMUM TEMPERATURE`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of observation: 958371  (--0)\n",
      "Reduction: 0  (0.0 %)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Attributes description:\n",
    "    AWND : Average wind speed\n",
    "\n",
    "    TMAX : Maximum temperature\n",
    "    TMIN : Minimum temperature\n",
    "\n",
    "    PRCP : Precipitation\n",
    "    WT16 : Rain(may include freezing rain, drizzle, and freezing drizzle)\"\n",
    "\n",
    "    SNOW : Snowfall\n",
    "    SNWD : Snow depth\n",
    "    WT18 : Snow, snow pellets, snow grains, or ice crystals\n",
    "\n",
    "    WT08 : Smoke or haze\n",
    "    WT22 : Ice fog or freezing fog\n",
    "    WT01 : Fog, ice fog, or freezing fog (may include heavy fog)\n",
    "    WT02 : Heavy fog or heaving freezing fog (not always distinguished from fog)\n",
    "    WT13 : Mist\n",
    "\n",
    "    WT06 : Glaze or rime\n",
    "\"\"\"\n",
    "\n",
    "\"\"\" Path \"\"\"\n",
    "fileName = 'weather.csv'\n",
    "filePath = os.path.abspath(os.path.join(os.getcwd(), fileName))\n",
    "\n",
    "\"\"\" Load \"\"\"\n",
    "weather =  pd.read_csv(filePath)\n",
    "\n",
    "\"\"\" Slice needed features for further investigation \"\"\"\n",
    "weather_features = (\n",
    "    ['DATE'] + # date\n",
    "    ['AWND'] + # wind related \n",
    "    ['TMAX','TMIN'] + # temp related\n",
    "    ['PRCP','WT16'] + # rain related\n",
    "    ['SNOW','SNWD','WT18'] + # snow related\n",
    "    ['WT08','WT22','WT01','WT02','WT13'] + # fog/vision related\n",
    "    ['WT06'] # rime related\n",
    "    )\n",
    "weather = weather[weather_features]\n",
    "weather = weather.fillna(0)\n",
    "\n",
    "\"\"\" prepare rain related features: \n",
    "        PRCP : Precipitation\n",
    "        WT16 : Rain(may include freezing rain, drizzle, and freezing drizzle)\"\n",
    "\"\"\"\n",
    "weather[['PRCP','WT16']]\n",
    "weather['PRCP'].value_counts().values\n",
    "weather['WT16'].value_counts() # 23 \n",
    "\n",
    "weather['Precipitation'.upper()] = weather['PRCP'].copy()\n",
    "weather = weather.drop(columns=['PRCP','WT16'])\n",
    "\n",
    "weather['Precipitation'.upper()].value_counts()\n",
    "\n",
    "\n",
    "\"\"\" prepare snow related features:\n",
    "        SNOW : Snowfall\n",
    "        SNWD : Snow depth\n",
    "        WT18 : Snow, snow pellets, snow grains, or ice crystals\n",
    "\"\"\"\n",
    "weather[['SNOW','SNWD','WT18']]\n",
    "weather['SNOW'].value_counts()\n",
    "weather['SNWD'].value_counts()\n",
    "weather['WT18'].value_counts() # 21\n",
    "\n",
    "weather['Snow fall'.upper()] = weather['SNOW'].copy()\n",
    "weather['Snow depth'.upper()] = weather['SNWD'].copy()\n",
    "weather = weather.drop(columns=['SNOW','SNWD','WT18'])\n",
    "\n",
    "weather['Snow fall'.upper()].value_counts()\n",
    "weather['Snow depth'.upper()].value_counts()\n",
    "\n",
    "\n",
    "\"\"\" prepare fog/vision related features:\n",
    "        WT01 : Fog, ice fog, or freezing fog (may include heavy fog)\n",
    "        WT08 : Smoke or haze\n",
    "        WT02 : Heavy fog or heaving freezing fog (not always distinguished from fog)\n",
    "        WT13 : Mist\n",
    "        WT22 : Ice fog or freezing fog\n",
    "\"\"\"\n",
    "weather[['WT08','WT22','WT01','WT02','WT13']]\n",
    "weather['WT01'].value_counts()\n",
    "weather['WT08'].value_counts()\n",
    "weather['WT02'].value_counts()\n",
    "weather['WT13'].value_counts() # 27\n",
    "weather['WT22'].value_counts() # 2\n",
    "\n",
    "weather['Fog, Smoke or haze'.upper()] = np.where(weather[['WT01','WT08','WT02']].sum(axis=1) == 0, 0, 1)\n",
    "weather = weather.drop(columns=['WT08','WT22','WT01','WT02','WT13'])\n",
    "\n",
    "weather['Fog, Smoke or haze'.upper()].value_counts()\n",
    "\n",
    "\n",
    "\"\"\" prepare rime related features \"\"\"\n",
    "\"\"\"\n",
    "    WT06 : Glaze or rime\n",
    "\"\"\"\n",
    "weather['WT06']\n",
    "weather['WT06'].value_counts() # 14\n",
    "weather = weather.drop(columns=['WT06'])\n",
    "\n",
    "\n",
    "\"\"\" Merage weather data with Data \"\"\"\n",
    "weather['DATE'] = pd.to_datetime(weather['DATE']).dt.date\n",
    "Data['DATE'] = pd.to_datetime(Data['CRASH DATE']).dt.date\n",
    "\n",
    "Data = pd.merge(Data, weather, on='DATE', how='left')\n",
    "Data = Data.drop(columns=['DATE'])\n",
    "\n",
    "\"\"\" Track Reduction in data \"\"\"\n",
    "reduc('Adding Weather')\n",
    "\n",
    "\"\"\" View and Rename Weather features \"\"\"\n",
    "Data['Average wind speed'.upper()] = Data['AWND'].copy()\n",
    "Data['Maximum temperature'.upper()] = Data['TMAX'].copy()\n",
    "Data['Minimum temperature'.upper()] = Data['TMIN'].copy()\n",
    "Data = Data.drop(columns=['AWND','TMAX','TMIN'])\n",
    "\n",
    "\"\"\" free memory \"\"\"\n",
    "del(weather)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \n",
    "---\n",
    "\n",
    "# <span style=\"color:MediumSlateBlue\">Save final Data:</span>\n",
    "\n",
    "---\n",
    "\n",
    "We save the final dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "fileName = 'MVC_SL_W_Final.csv'\n",
    "filePath = os.path.abspath(os.path.join(os.getcwd(), fileName))\n",
    "Data.to_csv(filePath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \n",
    "---\n",
    "\n",
    "# <span style=\"color:MediumSlateBlue\">Clear All Variables:</span>\n",
    "\n",
    "---\n",
    "\n",
    "To free up memory space, clear all variables!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
