{"cells":[{"source":["  ---\n","\n","  \n"," # <span style=\"color:MediumSlateBlue     \">Final Project | Explainer Notebook.</span>\n","\n","<span style=\"color:MediumSlateBlue     \">**02806 Social data analysis and visualization**</span>\n","\n","<span style=\"color:MediumSlateBlue     \">**May 2021**</span>\n","\n","<span style=\"color:MediumSlateBlue     \"> **Data-sets Reference: Motor-Vihecle-Collisions<sup>[link](https://data.cityofnewyork.us/Public-Safety/Motor-Vehicle-Collisions-Crashes/h9gi-nx95)</sup>, Weather-Data<sup>[link](https://www.ncdc.noaa.gov/cdo-web/search)</sup>, Speed-Limit-Data<sup>[link](https://data.cityofnewyork.us/Transportation/VZV_Speed-Limits/7n5j-865y)**</sup></span>\n","\n","  ---\n","  \n","\n","  \n","<span style=\"color:Orange\">**Please note!**</span> If you are using Jupyter to display this \".ipynb\" file You might need to make it *Trusted* in order to let Jupyter render the plots.\n","   "],"cell_type":"markdown","metadata":{}},{"source":["---\n","\n","# <span style=\"color:MediumSlateBlue\">Motivation.</span>\n","\n","---\n","\n","## What is your dataset?\n","NYC Motor Vehicle Collisions - Crashes<sup>[link](https://data.cityofnewyork.us/Public-Safety/Motor-Vehicle-Collisions-Crashes/h9gi-nx95)</sup>. It's freely available and has well defined Spatio-temporal information, as well as casualties and damages related features. Additionally, We will also consider the corresponding Weather<sup>[link](https://www.ncdc.noaa.gov/cdo-web/search)</sup> and Speed-Limit Data<sup>[link](https://data.cityofnewyork.us/Transportation/VZV_Speed-Limits/7n5j-865y)</sup></span>.\n","\n","\n","\n","## Why did you choose this/these particular dataset(s)?\n","Vehicle crashes happen daily around the globe. \n","They, for example, cost the New York City economy an enormous amount of $4 billion per year <sup>[link](https://nypost.com/2015/03/20car-accidents-cost-nyc-nearly-4-billion-a-year/)</sup></span>. Thus, it's might be beneficial to invistigate the chossen data to learn more about this phenomena and analyse the core reasons and contributing factors behind those accidents.\n","\n","## What was your goal for the end user's experience?\n","To give the end user the ability to investigate the data in an intractive way, where they can learn and build there own assumptions about this phenomena based on strong statistical analysis and visulizations."],"cell_type":"markdown","metadata":{}},{"cell_type":"markdown","metadata":{},"source":["\n","---\n","\n","# <span style=\"color:MediumSlateBlue\">Import needed libraries:</span>\n","\n","---\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\"\"\" IPython \"\"\"\n","from IPython.display import display\n","from IPython import get_ipython\n","get_ipython().run_line_magic('matplotlib', 'inline')\n","\n","\"\"\" Data Handeling \"\"\"\n","import numpy as np \n","import pandas as pd \n","from pandas import set_option\n","import calendar\n","import os \n","\n","\"\"\" Plot \"\"\" \n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","sns.set()\n","import folium\n","from bokeh.io import show\n","from bokeh.io import output_notebook\n","from bokeh.models import ColumnDataSource\n","from bokeh.models import FactorRange\n","from bokeh.models import Legend\n","from bokeh.plotting import figure\n","from bokeh import palettes\n","output_notebook() # open the bokeh viz on the notebook.\n","\n","\"\"\" for warnings \"\"\"\n","import warnings \n","warnings.simplefilter(\"ignore\")\n","\n","\"\"\" for Statistic \"\"\"\n","from scipy import stats\n","from scipy.stats import ks_2samp\n","\n","\"\"\" for Cross Validation \"\"\"\n","from sklearn.model_selection import train_test_split\n","from sklearn.model_selection import KFold\n","from sklearn.model_selection import RepeatedStratifiedKFold\n","from sklearn.model_selection import RandomizedSearchCV\n","from sklearn.model_selection import GridSearchCV\n","from sklearn.model_selection import cross_val_score\n","\n","\"\"\" for Evaluation Metrics \"\"\" \n","from sklearn.metrics import accuracy_score\n","from sklearn.metrics import classification_report\n","from sklearn.metrics import confusion_matrix\n","from sklearn.inspection import permutation_importance\n","\n","\"\"\" for Data Preprocessing  \"\"\"\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.preprocessing import OrdinalEncoder \n","from sklearn.impute import SimpleImputer\n","from sklearn.utils import resample\n","from sklearn.compose import ColumnTransformer\n","from sklearn.compose import make_column_selector as selector\n","\n","\"\"\" for Models \"\"\"\n","from sklearn.pipeline import Pipeline\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.tree import DecisionTreeClassifier\n","\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["\n","---\n","\n","# <span style=\"color:MediumSlateBlue\">Load data:</span>\n","\n","---\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\"\"\" Path \"\"\"\n","fileName = 'Motor_Vehicle_Collisions.csv'\n","filePath = os.path.abspath(os.path.join(os.getcwd(), fileName))\n","\n","\"\"\" Load \"\"\"\n","Data =  pd.read_csv(filePath);\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["\n","---\n","\n","# <span style=\"color:MediumSlateBlue\">Getting to know the Dataset:</span>\n","\n","---\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\"\"\" Define a function to track Reduction in data when doing the data prepration and cleaning later on \"\"\"\n","\n","Reduction = {}\n","Reduction_Percentage = {}\n","N = Data.shape[0]\n","def reduc(step):\n","    global N \n","    global Reduction\n","    global Reduction_Percentage\n","    N_before = N\n","    N_after = Data.shape[0]\n","    Reduction[step] = N_after\n","    Reduction_Percentage[step] = (N_before - N_after) / N_before\n","    print(f'Number of observation: {N_after}  (--{N_before-N_after})')\n","    print(f'Reduction: {N_before - N_after}  ({(N_before - N_after) / N_before} %)')\n","    N = Data.shape[0]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\"\"\" Initilize Reduction in data \"\"\"\n","reduc('Init')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\"\"\" Overview \"\"\"\n","Data.head(n=5)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\"\"\" Data shape \"\"\"\n","Data.shape"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\"\"\" Columns' names \"\"\"\n","Data.columns"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\"\"\" Columns types \"\"\"\n","Data.dtypes"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\"\"\" Count columns' NaN values in desending order \"\"\"\n","sorted(list(zip(Data.columns,Data.isna().sum(axis=0).values)) , key= lambda row: row[1], reverse=True)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\"\"\" Count columns' Non-NaN values in desending order \"\"\"\n","sorted(list(zip(Data.count().keys(),Data.count().values)), key= lambda row: row[1], reverse=True)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\"\"\" Count columns' zeros values \"\"\"\n","(Data == 0).sum(axis=0)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\"\"\" Count columns' empty strings \"\"\"\n","(Data == '').sum(axis=0)"]},{"cell_type":"markdown","metadata":{},"source":[" \n","---\n","\n","# <span style=\"color:MediumSlateBlue\">Data Cleaning:</span>\n","\n","---"]},{"cell_type":"markdown","metadata":{},"source":["## Drop unneeded features:"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\"\"\" Drop 'COLLISION_ID' since it's not informative \"\"\"\n","Data = Data.drop(columns=['COLLISION_ID'])\n","\n","\"\"\" Drop 'LOCATION' since we have 'LATITUDE', 'LONGITUDE' \"\"\"\n","Data = Data.drop(columns=['LOCATION'])\n","\n","\"\"\" Drop 'CROSS STREET NAME' and 'OFF STREET NAME' since we have 'ON STREET NAME' \"\"\"\n","Data = Data.drop(columns=['CROSS STREET NAME', 'OFF STREET NAME'])\n","\n","\"\"\" Drop PEDESTRIANS, CYCLIST and MOTORIST features since we have PERSONS features \"\"\"\n","Data = Data.drop(columns = ['NUMBER OF PEDESTRIANS INJURED','NUMBER OF PEDESTRIANS KILLED', \n","                            'NUMBER OF CYCLIST INJURED','NUMBER OF CYCLIST KILLED', \n","                            'NUMBER OF MOTORIST INJURED','NUMBER OF MOTORIST KILLED'])\n","\n","\"\"\" Consider only Collisions with two vehicles involve and Drop other unrelated features \"\"\"\n","Data = Data[\n","        (Data['CONTRIBUTING FACTOR VEHICLE 3'].isna())|\n","        (Data['CONTRIBUTING FACTOR VEHICLE 4'].isna())|\n","        (Data['CONTRIBUTING FACTOR VEHICLE 5'].isna())|\n","        (Data['VEHICLE TYPE CODE 3'].isna())|\n","        (Data['VEHICLE TYPE CODE 4'].isna())|\n","        (Data['VEHICLE TYPE CODE 5'].isna())]\n","Data = Data.drop(columns=['CONTRIBUTING FACTOR VEHICLE 3','CONTRIBUTING FACTOR VEHICLE 4','CONTRIBUTING FACTOR VEHICLE 5','VEHICLE TYPE CODE 3', 'VEHICLE TYPE CODE 4', 'VEHICLE TYPE CODE 5'])\n","\n","\"\"\" Track Reduction in data \"\"\"\n","reduc('MVC with only two vehicles involves')"]},{"cell_type":"markdown","metadata":{},"source":[" \n","---\n","\n","# <span style=\"color:MediumSlateBlue\">Missing Data:</span>\n","\n","---"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\"\"\" Count columns' NaN values in desending order \"\"\"\n","sorted(list(zip(Data.columns,Data.isna().sum(axis=0).values)) , key= lambda row: row[1], reverse=True)\n","\n","\"\"\" Count columns' zeros values \"\"\"\n","(Data == 0).sum(axis=0)\n","\n","\"\"\" Count columns' empty strings \"\"\"\n","(Data == '').sum(axis=0)\n","\n","\"\"\" Drop rows that has a messing value in one of important features \"\"\"\n","Data = Data[\n","    Data['ON STREET NAME'].notna()  & # important feature for adding speed limit data later on.\n","    Data['LATITUDE' ].notna()       & # imporatnt feature for map plots \n","    Data['LONGITUDE'].notna()       & # imporatnt feature for map plots\n","    Data['NUMBER OF PERSONS INJURED'].notna()   & # imporatnt feature since one of the main features of intress\n","    Data['NUMBER OF PERSONS KILLED'].notna()      # imporatnt feature since one of the main features of intress\n","    ].copy()\n","\n","\"\"\" Drop raws with LATITUDE or LONGITUDE = 0 \"\"\"\n","Data = Data[(Data['LATITUDE']!=0)|(Data['LONGITUDE']!=0)].copy()\n","\n","\"\"\" Track Reduction in data \"\"\"\n","reduc('Drop missing values in important features')"]},{"cell_type":"markdown","metadata":{},"source":[" \n","---\n","\n","# <span style=\"color:MediumSlateBlue\">Features Prepration:</span>\n","\n","---"]},{"cell_type":"markdown","metadata":{},"source":["## Prepare Vehicle types:"]},{"cell_type":"markdown","metadata":{},"source":[" **Prepare Vehicle type 1:**"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\"\"\" Unify Vehicle type recording way \"\"\"\n","Data['VEHICLE TYPE CODE 1'] = Data['VEHICLE TYPE CODE 1'].str.lower()\n","Data['VEHICLE TYPE CODE 1'] = Data['VEHICLE TYPE CODE 1'].str.strip()\n","\n","\"\"\" Fixing recording issus of Vehicle types that has more than 50 MVC occurrences \"\"\"\n","Frequent_MVC_Vehicles = (Data['VEHICLE TYPE CODE 1'].value_counts().keys()[Data['VEHICLE TYPE CODE 1'].value_counts().values > 50])\n","\n","Mapping = { # Basesd on Frequent_MVC_Vehicles values\n","    np.nan: 'unknown',\n","    'station wagon/sport utility vehicle': 'sport utility vehicle', \n","    'sport utility / station wagon':'sport utility vehicle', \n","    '4 dr sedan': 'sedan', \n","    'ambul': 'ambulance',  \n","    'school bus': 'school bus', \n","    'e-sco': 'e-scooter', \n","    'schoo': 'school bus', \n","    'bicycle': 'bike'\n","    }\n","\n","Data['VEHICLE TYPE CODE 1'] = Data['VEHICLE TYPE CODE 1'].replace(Mapping)\n","\n","\"\"\" Consider only 95 % Frequent MVC Vehicle types \"\"\"\n","VT1 = pd.DataFrame()\n","VT1['VEHICLE TYPE CODE 1'] = Data['VEHICLE TYPE CODE 1'].value_counts(normalize=True).keys()\n","VT1['Frequencies'] = Data['VEHICLE TYPE CODE 1'].value_counts(normalize=True).values\n","\n","threshold = 0\n","for i in range(len(VT1['VEHICLE TYPE CODE 1'].unique())):\n","    Sum = VT1['Frequencies'][0:i+1].sum()\n","    if Sum > 0.95:\n","         threshold = i + 1\n","        #  print(\"Threshold that covers 95% of \" + \"VEHICLE TYPEs\".lower() +  \" = \" + f\"{threshold}\")\n","         break \n","Focus_Vehicles_Type_1 = list(VT1['VEHICLE TYPE CODE 1'][0:threshold].values)\n","print('Focus Vehicles Type 1 (95 % Frequent):')\n","print(Focus_Vehicles_Type_1)\n","print()\n"]},{"cell_type":"markdown","metadata":{},"source":["**Prepare Vehicle type 2:**"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\"\"\" Unify Vehicle type recording way \"\"\"\n","Data['VEHICLE TYPE CODE 2'] = Data['VEHICLE TYPE CODE 2'].str.lower()\n","Data['VEHICLE TYPE CODE 2'] = Data['VEHICLE TYPE CODE 2'].str.strip()\n","\n","\"\"\" Fixing recording issus of Vehicle types that has more than 50 MVC occurrences \"\"\"\n","Frequent_MVC_Vehicles = (Data['VEHICLE TYPE CODE 2'].value_counts().keys()[Data['VEHICLE TYPE CODE 2'].value_counts().values > 50])\n","\n","Mapping = { # Based on Frequent_MVC_Vehicles values\n","    np.nan: 'unknown',\n","    'unkno': 'unknown',\n","    'unk': 'unknown',\n","    'station wagon/sport utility vehicle': 'sport utility vehicle', \n","    'sport utility / station wagon':'sport utility vehicle', \n","    '4 dr sedan': 'sedan', \n","    'ambul': 'ambulance',  \n","    'school bus': 'school bus', \n","    'e-sco': 'e-scooter', \n","    'schoo': 'school bus', \n","    'bicycle': 'bike', \n","    }\n","\n","Data['VEHICLE TYPE CODE 2'] = Data['VEHICLE TYPE CODE 2'].replace(Mapping)\n","\n","\"\"\" Consider only 95 % Frequent MVC Vehicle types \"\"\"\n","VT2 = pd.DataFrame()\n","VT2['VEHICLE TYPE CODE 2'] = Data['VEHICLE TYPE CODE 2'].value_counts(normalize=True).keys()\n","VT2['Frequencies'] = Data['VEHICLE TYPE CODE 2'].value_counts(normalize=True).values\n","\n","threshold = 0\n","for i in range(len(VT2['VEHICLE TYPE CODE 2'].unique())):\n","    Sum = VT2['Frequencies'][0:i+1].sum()\n","    if Sum > 0.95:\n","         threshold = i + 1\n","        #  print(\"Threshold that cover 95% of \" + \"VEHICLE TYPEs\".lower() +  \" = \" + f\"{threshold}\")\n","         break \n","Focus_Vehicles_Type_2 = VT2['VEHICLE TYPE CODE 2'][0:threshold].values\n","\n","print('Focus Vehicles Type 2 (95 % Frequent):')\n","print(Focus_Vehicles_Type_2)\n","print()"]},{"cell_type":"markdown","metadata":{},"source":["**Slice Focus Vehicle Types (covers more than 95 % of MVC occurrences)**"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\"\"\" Slice \"\"\"\n","Focus_Vehicle_Types = list(set(list(Focus_Vehicles_Type_1) + list(Focus_Vehicles_Type_2))) \n","Data = Data[Data['VEHICLE TYPE CODE 1'].isin((Focus_Vehicle_Types)) & (Data['VEHICLE TYPE CODE 2'].isin(Focus_Vehicle_Types))].copy()\n","print('Focus Vehicles ( More than 95 % Frequent):')\n","print(Focus_Vehicle_Types)\n","print()\n","\n","\"\"\" Track Reduction in data \"\"\"\n","reduc('Slice Focus Vehicle Types')\n","\n","\"\"\" free memory \"\"\"\n","del(VT1,VT2)"]},{"cell_type":"markdown","metadata":{},"source":["## Prepare Contributing Factors:"]},{"cell_type":"markdown","metadata":{},"source":["**Prepare Contributing Factor 1:**"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\"\"\" Unify Contributing Factor string \"\"\"\n","Data['CONTRIBUTING FACTOR VEHICLE 1'] = Data['CONTRIBUTING FACTOR VEHICLE 1'].str.lower()\n","Data['CONTRIBUTING FACTOR VEHICLE 1'] = Data['CONTRIBUTING FACTOR VEHICLE 1'].str.strip()\n","\n","\"\"\" Fixing recording issus of Contributing Factor that has more than 50 MVC occurrences \"\"\"\n","Frequent_MVC_Factors = (Data['CONTRIBUTING FACTOR VEHICLE 1'].value_counts().keys()[Data['CONTRIBUTING FACTOR VEHICLE 1'].value_counts().values > 50])\n","\n","Mapping = { # Based on Frequent_MVC_Factors\n","    np.nan: 'unknown',\n","    'illnes':'illness', \n","    'reaction to other uninvolved vehicle':'reaction to uninvolved vehicle',\n","    'passing too closely': 'passing or lane usage improper',\n","    }\n","\n","Data['CONTRIBUTING FACTOR VEHICLE 1'] = Data['CONTRIBUTING FACTOR VEHICLE 1'].replace(Mapping)\n","\n","\"\"\" Consider only 95 % Frequent MVC Contributing Factors \"\"\"\n","CF1 = pd.DataFrame()\n","CF1['CONTRIBUTING FACTOR VEHICLE 1'] = Data['CONTRIBUTING FACTOR VEHICLE 1'].value_counts(normalize=True).keys()\n","CF1['Frequencies'] = Data['CONTRIBUTING FACTOR VEHICLE 1'].value_counts(normalize=True).values\n","\n","threshold = 0\n","for i in range(len(CF1['CONTRIBUTING FACTOR VEHICLE 1'].unique())):\n","    Sum = CF1['Frequencies'][0:i+1].sum()\n","    if Sum > 0.95:\n","         threshold = i + 1\n","        #  print(\"Threshold that covers 95% of \" + \"CONTRIBUTING FACTORs\".lower() +  \" = \" + f\"{threshold}\")\n","         break \n","Focus_Factors_Type_1 = list(CF1['CONTRIBUTING FACTOR VEHICLE 1'][0:threshold].values)\n","print('Focus Factors 1 (95 % Frequent):')\n","print(Focus_Factors_Type_1)\n","print()"]},{"cell_type":"markdown","metadata":{},"source":["**Prepare Contributing Factor 2:**"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\"\"\" Unify Contributing Factor string \"\"\"\n","Data['CONTRIBUTING FACTOR VEHICLE 2'] = Data['CONTRIBUTING FACTOR VEHICLE 2'].str.lower()\n","Data['CONTRIBUTING FACTOR VEHICLE 2'] = Data['CONTRIBUTING FACTOR VEHICLE 2'].str.strip()\n","\n","\"\"\" Fixing recording issus of Contributing Factor that has more than 50 MVC occurrences \"\"\"\n","Frequent_MVC_Factors = (Data['CONTRIBUTING FACTOR VEHICLE 2'].value_counts().keys()[Data['CONTRIBUTING FACTOR VEHICLE 2'].value_counts().values > 50])\n","\n","Mapping = { # Based on Frequent_MVC_Factors\n","    np.nan: 'unknown',\n","    'illnes':'illness', \n","    'reaction to other uninvolved vehicle':'reaction to uninvolved vehicle',\n","    'passing too closely': 'passing or lane usage improper',\n","    }\n","\n","Data['CONTRIBUTING FACTOR VEHICLE 2'] = Data['CONTRIBUTING FACTOR VEHICLE 2'].replace(Mapping)\n","\n","\"\"\" Consider only 95 % Frequent MVC Contributing Factors \"\"\"\n","CF2 = pd.DataFrame()\n","CF2['CONTRIBUTING FACTOR VEHICLE 2'] = Data['CONTRIBUTING FACTOR VEHICLE 2'].value_counts(normalize=True).keys()\n","CF2['Frequencies'] = Data['CONTRIBUTING FACTOR VEHICLE 2'].value_counts(normalize=True).values\n","\n","threshold = 0\n","for i in range(len(CF2['CONTRIBUTING FACTOR VEHICLE 2'].unique())):\n","    Sum = CF2['Frequencies'][0:i+1].sum()\n","    if Sum > 0.95:\n","         threshold = i + 1\n","        #  print(\"Threshold that covers 95% of \" + \"CONTRIBUTING FACTORs\".lower() +  \" = \" + f\"{threshold}\")\n","         break \n","Focus_Factors_Type_2 = list(CF2['CONTRIBUTING FACTOR VEHICLE 2'][0:threshold].values)\n","print('Focus Factors 2 (95 % Frequent):')\n","print(Focus_Factors_Type_2)\n","print()"]},{"cell_type":"markdown","metadata":{},"source":["**Slice Focus Factors Type (covers more than 95 % of MVC occurrences)**"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\"\"\" Slice \"\"\"\n","Focus_Factors_Types = list(set(list(Focus_Factors_Type_1) + list(Focus_Factors_Type_2))) \n","Data = Data[Data['CONTRIBUTING FACTOR VEHICLE 1'].isin((Focus_Factors_Types)) & (Data['CONTRIBUTING FACTOR VEHICLE 2'].isin(Focus_Factors_Types))].copy()\n","print('Focus Factors ( More than 95 % Frequent):')\n","print(Focus_Factors_Types)\n","print()\n","\n","\"\"\" Track Reduction in data \"\"\"\n","reduc('Slice Focus Factors Types')\n","\n","\"\"\" free memory \"\"\"\n","del(CF1,CF2)"]},{"cell_type":"markdown","metadata":{},"source":["## Prepare Zip Features:"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\"\"\" Drop Unspecified Zip \"\"\"\n","Data['ZIP CODE'].replace(to_replace='     ', value=np.nan, inplace=True)\n","\n","\"\"\" Change the Zip type to float64 \"\"\" \n","Data['ZIP CODE'] = pd.to_numeric(Data['ZIP CODE']) "]},{"cell_type":"markdown","metadata":{},"source":[" ##  Extract new feutres:"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\"\"\" Add the 'Respone' feature, which is a binary future that says 0 if there is no injures or killed person and 1 other wise. \"\"\"\n","Data['Response'] = Data[['NUMBER OF PERSONS INJURED','NUMBER OF PERSONS KILLED']].sum(axis=1)\n","Data['Response'] = Data['Response'].apply(lambda y: 1 if y > 0 else 0)\n","\n","\"\"\" Add 'Year' feature \"\"\"\n","Data['Year']    = pd.to_datetime(Data['CRASH DATE']).dt.year\n","\n","\"\"\" Add 'Month' feature \"\"\"\n","Data['Month']    = pd.to_datetime(Data['CRASH DATE']).dt.month\n","\n","\"\"\" Add 'Day' feature \"\"\"\n","Data['Day'] = pd.to_datetime(Data['CRASH DATE']).dt.day\n","\n","\"\"\" 'Day of week' feature \"\"\"\n","Data['Day of week'] = pd.to_datetime(Data['CRASH DATE']).dt.day_name()\n","\n","\"\"\" Add 'Hour' feature \"\"\"\n","Data['Hour'] = pd.to_datetime(Data['CRASH TIME']).dt.hour\n","\n","\"\"\" Add 'Minute' feature \"\"\"\n","Data['Minute'] = pd.to_datetime(Data['CRASH TIME']).dt.minute"]},{"cell_type":"markdown","metadata":{},"source":["## Drop uncompleted years:"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\"\"\" Drop rows from 2012 since they are not completed  \"\"\"\n","Data = Data[Data['Year']!=2012]\n","\n","\"\"\" Drop rows from 2021 since they are not completed  \"\"\"\n","Data = Data[Data['Year']!=2021]\n","\n","\"\"\" Track Reduction in data \"\"\"\n","reduc('Drop uncompleted years')"]},{"cell_type":"markdown","metadata":{},"source":[" \n","---\n","\n","# <span style=\"color:MediumSlateBlue\">Adding new Datasets:</span>\n","\n","---"]},{"cell_type":"markdown","metadata":{},"source":["## Adding Speed_Limits Mode Data:"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\"\"\" path \"\"\"\n","fileName = 'dot_VZV_Speed_Limits_20210507.csv'\n","filePath = os.path.abspath(os.path.join(os.getcwd(), fileName))\n","\n","\"\"\" load \"\"\"\n","speed_limits =  pd.read_csv(filePath)\n","\n","\"\"\" Drop speed limits rows with missing values in important features \"\"\"\n","speed_limits = speed_limits[\n","        speed_limits['street'].notna()  &\n","        speed_limits['postvz_sl'].notna()  \n","    ].copy()\n","\n","\"\"\" Prepare street name features of both datasets for merging \"\"\"\n","Data.loc[:,'ON STREET NAME'] = Data['ON STREET NAME'].str.lower()\n","Data.loc[:,'ON STREET NAME'] = Data['ON STREET NAME'].str.strip()\n","speed_limits.loc[:,'street'] = speed_limits['street'].str.lower()\n","speed_limits.loc[:,'street'] = speed_limits['street'].str.strip()\n","\n","Matched_streets = Data['ON STREET NAME'][Data['ON STREET NAME'].isin(speed_limits['street'])].unique()\n","print('Merging Speed Limits:') \n","print(f\"    Number of Matched Streets = {len(Matched_streets)}\")\n","print(f\"    Number of Unmatched Streets = {len(Data['ON STREET NAME'].unique()) - len(Matched_streets)}\")\n","\n","\"\"\" Calculate speed limits mode\"\"\"\n","Street_Speed_Mode = {}\n","streets = speed_limits['street'].unique()\n","for street in streets:\n","    Street_Values = speed_limits[speed_limits['street']==street]['postvz_sl']\n","    Street_Mode = stats.mode(Street_Values)[0][0]\n","    Street_Speed_Mode[street]= Street_Mode\n","\n","\"\"\" Add speed limits mode to Data \"\"\"\n","Data = Data[Data['ON STREET NAME'].isin(streets)].copy()\n","Data['SPEED LIMIT MODE'] = Data['ON STREET NAME'].apply(lambda street: Street_Speed_Mode[street])\n","\n","\"\"\" Track Reduction in data \"\"\"\n","reduc('Adding Speed_Limits')\n","\n","\"\"\" Free memory \"\"\"\n","del(speed_limits,Matched_streets, Street_Speed_Mode, streets)"]},{"cell_type":"markdown","metadata":{},"source":[" ## Adding weather data:"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\"\"\"\n","Attributes description:\n","    AWND : Average wind speed\n","\n","    TMAX : Maximum temperature\n","    TMIN : Minimum temperature\n","\n","    PRCP : Precipitation\n","    WT16 : Rain(may include freezing rain, drizzle, and freezing drizzle)\"\n","\n","    SNOW : Snowfall\n","    SNWD : Snow depth\n","    WT18 : Snow, snow pellets, snow grains, or ice crystals\n","\n","    WT08 : Smoke or haze\n","    WT22 : Ice fog or freezing fog\n","    WT01 : Fog, ice fog, or freezing fog (may include heavy fog)\n","    WT02 : Heavy fog or heaving freezing fog (not always distinguished from fog)\n","    WT13 : Mist\n","\n","    WT06 : Glaze or rime\n","\"\"\"\n","\n","\"\"\" Path \"\"\"\n","fileName = 'weather.csv'\n","filePath = os.path.abspath(os.path.join(os.getcwd(), fileName))\n","\n","\"\"\" Load \"\"\"\n","weather =  pd.read_csv(filePath)\n","\n","\"\"\" Slice needed features for further investigation \"\"\"\n","weather_features = (\n","    ['DATE'] + # date\n","    ['AWND'] + # wind related \n","    ['TMAX','TMIN'] + # temp related\n","    ['PRCP','WT16'] + # rain related\n","    ['SNOW','SNWD','WT18'] + # snow related\n","    ['WT08','WT22','WT01','WT02','WT13'] + # fog/vision related\n","    ['WT06'] # rime related\n","    )\n","weather = weather[weather_features]\n","weather = weather.fillna(0)\n","\n","\"\"\" prepare rain related features: \n","        PRCP : Precipitation\n","        WT16 : Rain(may include freezing rain, drizzle, and freezing drizzle)\"\n","\"\"\"\n","weather[['PRCP','WT16']]\n","weather['PRCP'].value_counts().values\n","weather['WT16'].value_counts() # 23 \n","\n","weather['Precipitation'.upper()] = weather['PRCP'].copy()\n","weather = weather.drop(columns=['PRCP','WT16'])\n","\n","weather['Precipitation'.upper()].value_counts()\n","\n","\n","\"\"\" prepare snow related features:\n","        SNOW : Snowfall\n","        SNWD : Snow depth\n","        WT18 : Snow, snow pellets, snow grains, or ice crystals\n","\"\"\"\n","weather[['SNOW','SNWD','WT18']]\n","weather['SNOW'].value_counts()\n","weather['SNWD'].value_counts()\n","weather['WT18'].value_counts() # 21\n","\n","weather['Snow fall'.upper()] = weather['SNOW'].copy()\n","weather['Snow depth'.upper()] = weather['SNWD'].copy()\n","weather = weather.drop(columns=['SNOW','SNWD','WT18'])\n","\n","weather['Snow fall'.upper()].value_counts()\n","weather['Snow depth'.upper()].value_counts()\n","\n","\n","\"\"\" prepare fog/vision related features:\n","        WT01 : Fog, ice fog, or freezing fog (may include heavy fog)\n","        WT08 : Smoke or haze\n","        WT02 : Heavy fog or heaving freezing fog (not always distinguished from fog)\n","        WT13 : Mist\n","        WT22 : Ice fog or freezing fog\n","\"\"\"\n","weather[['WT08','WT22','WT01','WT02','WT13']]\n","weather['WT01'].value_counts()\n","weather['WT08'].value_counts()\n","weather['WT02'].value_counts()\n","weather['WT13'].value_counts() # 27\n","weather['WT22'].value_counts() # 2\n","\n","weather['Fog, Smoke or haze'.upper()] = np.where(weather[['WT01','WT08','WT02']].sum(axis=1) == 0, 0, 1)\n","weather = weather.drop(columns=['WT08','WT22','WT01','WT02','WT13'])\n","\n","weather['Fog, Smoke or haze'.upper()].value_counts()\n","\n","\n","\"\"\" prepare rime related features \"\"\"\n","\"\"\"\n","    WT06 : Glaze or rime\n","\"\"\"\n","weather['WT06']\n","weather['WT06'].value_counts() # 14\n","weather = weather.drop(columns=['WT06'])\n","\n","\n","\"\"\" Merage weather data with Data \"\"\"\n","weather['DATE'] = pd.to_datetime(weather['DATE']).dt.date\n","Data['DATE'] = pd.to_datetime(Data['CRASH DATE']).dt.date\n","\n","Data = pd.merge(Data, weather, on='DATE', how='left')\n","Data = Data.drop(columns=['DATE'])\n","\n","\"\"\" Track Reduction in data \"\"\"\n","reduc('Adding Weather')\n","\n","\"\"\" View and Rename Weather features \"\"\"\n","Data['Average wind speed'.upper()] = Data['AWND'].copy()\n","Data['Maximum temperature'.upper()] = Data['TMAX'].copy()\n","Data['Minimum temperature'.upper()] = Data['TMIN'].copy()\n","Data = Data.drop(columns=['AWND','TMAX','TMIN'])\n","\n","\"\"\" free memory \"\"\"\n","del(weather)"]},{"cell_type":"markdown","metadata":{},"source":[" \n","---\n","\n","# <span style=\"color:MediumSlateBlue\">Save final Data:</span>\n","\n","---\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["fileName = 'MVC_SL_W_Final.csv'\n","filePath = os.path.abspath(os.path.join(os.getcwd(), fileName))\n","Data.to_csv(filePath)"]},{"source":["---\n","\n","# <span style=\"color:MediumSlateBlue\">Genre:</span>\n","\n","---\n","\n","Asterios :)"],"cell_type":"markdown","metadata":{}},{"source":["---\n","\n","# <span style=\"color:MediumSlateBlue\">Basic Stats:</span>\n","\n","---"],"cell_type":"markdown","metadata":{}},{"cell_type":"markdown","metadata":{},"source":["## Summary Statistics:"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\"\"\" for categorical features \"\"\"\n","Desc_O = Data.select_dtypes(include=object).describe()\n","display(Desc_O)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\"\"\" for non-object features \"\"\"\n","Desc_N = Data.select_dtypes(exclude=object).describe()\n","Desc_N.loc['range'] = Desc_N.loc['max'] - Desc_N.loc['min']\n","Desc_N = Desc_N.append( Data.select_dtypes(exclude=object)\n","                            .mode()\n","                            .rename({0: 'mode'}, axis='index'))\n","set_option('precision', 2)\n","display(Desc_N)"]},{"cell_type":"markdown","metadata":{},"source":["## Some interesting counts:"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\"\"\" Number of MVC injured persons 2013 - 2020 \"\"\"\n","Data['NUMBER OF PERSONS INJURED'].sum()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\"\"\" Number of MVC killed persons 2013 - 2020 \"\"\"\n","Data['NUMBER OF PERSONS KILLED'].sum()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\"\"\" Number Respone since 2013 - 2020 \"\"\"\n","Data['Response'].sum()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\"\"\" Number of No-Respone since 2013 - 2020 \"\"\"\n","(Data['Response'] == 0).sum()\n"]},{"cell_type":"markdown","metadata":{},"source":[" \n","---\n","\n","# <span style=\"color:MediumSlateBlue\">Data Analysis and Visulization:</span>\n","\n","---\n"]},{"cell_type":"markdown","metadata":{},"source":["## Box and whisker plots"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\"\"\" For MVC Data \"\"\"\n","Box_lst = ['ZIP CODE', 'LATITUDE', 'LONGITUDE', 'NUMBER OF PERSONS INJURED','NUMBER OF PERSONS KILLED', 'Response', 'Year', 'Month', 'Hour','Minute']\n","Data[Box_lst].plot(\n","                kind='box', \n","                subplots=True, \n","                sharex=False, \n","                sharey=False, \n","                fontsize=10, \n","                layout=(4,3), \n","                figsize=(10,9),\n","                title='Box-Plot for MVC data'\n","                )\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\"\"\" For Speed Limits and Weather data \"\"\"\n","Box_lst = ['SPEED LIMIT MODE', 'PRECIPITATION', 'SNOW FALL','SNOW DEPTH', 'FOG, SMOKE OR HAZE', 'AVERAGE WIND SPEED','MAXIMUM TEMPERATURE', 'MINIMUM TEMPERATURE']\n","Data[Box_lst].plot(\n","                kind='box', \n","                subplots=True, \n","                sharex=False, \n","                sharey=False, \n","                fontsize=10, \n","                layout=(3,3), \n","                figsize=(10,7),\n","                title='Box-Plot for Speed Limits and Weather data'\n","                )\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["## Respone over Years - Plot"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["grouping = Data.groupby(['Response','Year']).count()['CRASH DATE']\n","\n","fig, axs = plt.subplots(1, 2,figsize=(10, 5))\n","fig.suptitle('Development of Respone over Years')\n","\n","axs.flat[0].set_title('NYC MVC Without injures or kills')\n","axs.flat[0].set_ylabel('Number of MVC')\n","grouping[0].plot(ax=axs.flat[0], color= 'tab:gray')    \n","\n","axs.flat[1].set_title('NYC MVC With injures or kills')\n","grouping[1].plot(ax=axs.flat[1], color= 'tab:gray')    \n","axs.flat[1].set_ylabel('Number of MVC')\n","\n","plt.show;"]},{"cell_type":"markdown","metadata":{},"source":["## Correlation Matrix Plot"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["fig = plt.figure(figsize=(10,10))\n","ax = fig.add_subplot(111)\n","cax = ax.matshow(Data.corr(), vmin=-1, vmax=1, interpolation='none')\n","fig.colorbar(cax)\n","ticks = np.arange(0,len(Data.select_dtypes(exclude=object).columns),1)\n","ax.set_xticks(ticks)\n","ax.set_yticks(ticks)\n","ax.set_xticklabels(['Zip', 'LAT.', 'LONG.', 'Injured','killed', 'Response', 'Year', 'Month', 'Day', 'Hour','Minute', 'S.L.M.', 'Pre.', 'S.F.','S. D.', 'F.S.H.', 'A.W.S.','Max. T.', 'Min. T.'], rotation=90)\n","ax.set_yticklabels(['Zip', 'LAT.', 'LONG.', 'Injured','killed', 'Response', 'Year', 'Month', 'Day', 'Hour','Minute', 'S.L.M.', 'Pre.', 'S.F.','S. D.', 'F.S.H.', 'A.W.S.','Max. T.', 'Min. T.'], rotation=0)\n","plt.grid(False)\n","plt.title('Correlation Matrix')\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["## Jitter-plots:"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\"\"\" Init filtered data for Jitter plotS \"\"\" \n","month = 1\n","hour = 13\n","\n","\"\"\" Jitter plot 01 : No-Response \"\"\"\n","data_Jit = Data[Data['Response']==0] \n","data_Jit = data_Jit[data_Jit['Month']==month]\n","data_Jit = data_Jit[data_Jit['Hour']==hour].reset_index(drop=True)\n","plt.figure(figsize=(7, 4))\n","sns.stripplot(data_Jit['Minute'].values, jitter=True, edgecolor='none', alpha=.50 ,color='k')\n","plt.title('NYC MVC without injuries or kills\\nMonth = '+ str(month) + '\\nHour = ' + str(hour) + '-'+ str(hour+1))\n","plt.show()\n","\n","\"\"\" Jitter plot 02 : With-Response \"\"\"\n","data_Jit = Data[Data['Response']==1] \n","data_Jit = data_Jit[data_Jit['Month']==1]\n","data_Jit = data_Jit[data_Jit['Hour']==13]\n","plt.figure(figsize=(7, 4))\n","sns.stripplot(data_Jit['Minute'].values, jitter=True, edgecolor='none', alpha=.50 ,color='k')\n","plt.title('NYC MVC with injuries or kills\\nMonth = '+ str(month) + '\\nHour = ' + str(hour) + '-'+ str(hour+1))\n","plt.show()\n","\n","\"\"\" free memory \"\"\"\n","del(data_Jit)"]},{"cell_type":"markdown","metadata":{},"source":["##  Histogram-Plots:"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\"\"\" Init filtered data for histogram plot \"\"\" \n","month = 1\n","\n","\"\"\" histogram 01: No-Respone \"\"\"\n","data_Hist = Data[Data['Response']==0]\n","data_Hist = data_Hist[data_Hist['Month'] == month]\n","plt.figure(figsize=(7, 5))\n","plt.hist(data_Hist['LATITUDE'],bins= 50) \n","plt.title(\"NYC MVC\\nwithout injuries or kills\\nMonth = \" + str(month))\n","plt.xlabel(\"Latitude\")\n","plt.ylabel(\"Number of Observations\")\n","plt.show()                      \n","\n","\"\"\" histogram 02: With-Respone \"\"\"\n","data_Hist = Data[Data['Response']==1]\n","data_Hist = data_Hist[data_Hist['Month'] == month]\n","plt.figure(figsize=(7, 5))\n","plt.hist(data_Hist['LATITUDE'], bins=50) \n","plt.title(\"NYC MVC\\nwith injuries or kills\\nMonth = \" + str(month))\n","plt.xlabel(\"Latitude\")\n","plt.ylabel(\"Number of Observations\")\n","plt.show()\n","\n","\"\"\" free memory \"\"\"\n","del(data_Hist)"]},{"cell_type":"markdown","metadata":{},"source":[" ## Map-plot:"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\"\"\" Init filtered data for map plot \"\"\" \n","select_month    = 1\n","start_year      = 2018\n","end_year        = 2019\n","\n","\"\"\" Map data \"\"\"\n","data_Map = Data[\n","            (Data['Month'] == select_month)&\n","            (Data['Year'] >= start_year)&\n","            (Data['Year']  < end_year)\n","            ].reset_index(drop=True)\n","\n","\"\"\" Create a NYC Map instances \"\"\"\n","MapNYC = folium.Map(\n","            location = [40.730610, -73.935242], \n","            tiles = 'Stamen Toner',\n","            zoom_start = 12)\n","\n","\"\"\" Add Marker for the City Hall to Map\"\"\"\n","folium.Marker(\n","    location = [40.712772, -74.006058],\n","    popup = 'City Hall',\n","    icon = folium.Icon( \n","                color='blue',\n","                icon='university',\n","                prefix='fa')).add_to(MapNYC)\n","\n","\"\"\" Start adding points \"\"\"\n","for i, row in data_Map.iterrows():\n","    if(row['Response']==1):\n","        folium.CircleMarker(\n","            location = [row['LATITUDE'], row['LONGITUDE']],\n","            radius=1,\n","            popup='Either Injure or Kill Occurred\\nin ' + str(row['CRASH DATE']) +\"\\nat \" + str(row['CRASH TIME']),\n","            color='red',\n","            opacity=0.5).add_to(MapNYC)\n","\n","    else:\n","        folium.CircleMarker(\n","            location = [row['LATITUDE'], row['LONGITUDE']],\n","            radius=1,\n","            popup='Neither Injure nor Kill Occurred\\nin ' + str(row['CRASH DATE']) +\"\\nat \" + str(row['CRASH TIME']),\n","            color='blue',\n","            opacity=0.5).add_to(MapNYC)\n","\n","\"\"\" Display Map\"\"\"\n","display(MapNYC)\n","\n","\"\"\" Free memory \"\"\"\n","del(data_Map)\n","\n"]},{"cell_type":"markdown","metadata":{},"source":[" ## Bokeh-Plot:"]},{"cell_type":"markdown","metadata":{},"source":[" Define a general **Bokeh-plot function**, for all Contributing Factors and Vehicle Types:"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def Bokeh_plot(plotMe):\n","    # Function Descreption:  \n","    \"\"\"\n","    Bokeh_plot(plotMe): \n","        # A general Bokeh-plot function, for all Contributing Factors and Vehicle Types.\n","        # Takes plotMe:String, with one of the corresponding possible values: \n","            - 'VEHICLE TYPE CODE 1'\n","            - 'VEHICLE TYPE CODE 2'\n","            - 'CONTRIBUTING FACTOR VEHICLE 1'\n","            - 'CONTRIBUTING FACTOR VEHICLE 2'\n","        # Returns:\n","            Bokeh plot or Error for invalid PlotMe value\n","    \"\"\"\n","\n","    # Check parameter values: \n","    if not( (plotMe == 'VEHICLE TYPE CODE 1') | \n","            (plotMe == 'VEHICLE TYPE CODE 2') |\n","            (plotMe == 'CONTRIBUTING FACTOR VEHICLE 1') | \n","            (plotMe == 'CONTRIBUTING FACTOR VEHICLE 2')\n","        ): # Notice the sad face, COOL! \n","        raise TypeError(\n","            \"Not allowed parameter value for 'plotMe' in function 'Bokeh_plot'\\n\" +\n","            \"The allowed parameter values are:\\n\" +\n","            \"   - 'VEHICLE TYPE CODE 1'\\n\" +\n","            \"   - 'VEHICLE TYPE CODE 2'\\n\" +\n","            \"   - 'CONTRIBUTING FACTOR VEHICLE 1'\\n\" +\n","            \"   - 'CONTRIBUTING FACTOR VEHICLE 2'\\n\"\n","        )\n","\n","    # Define parameter corresponding Focus list:\n","    Focus = []\n","    if ((plotMe == 'VEHICLE TYPE CODE 1') | (plotMe == 'VEHICLE TYPE CODE 2')):\n","        Focus = Focus_Vehicle_Types\n","    else:\n","        Focus = Focus_Factors_Types\n","\n","    # Define parameter corresponding Figure height and width:\n","    plot_height,plot_width = 0,0 \n","    if ((plotMe == 'VEHICLE TYPE CODE 1') | (plotMe == 'VEHICLE TYPE CODE 2')):\n","        plot_height=400\n","        plot_width=800\n","    else:\n","        plot_height=550\n","        plot_width=800\n","\n","    # Pivot Data (Table) for Bokeh:\n","    Table = pd.pivot_table(Data, \n","                        index = 'Hour', \n","                        columns = plotMe,\n","                        values = 'CRASH DATE',\n","                        aggfunc = 'count')\n","\n","    # Normalize: (div by sum)\n","    Table = Table.div(Table.sum(axis=0), axis=1)\n","\n","    # Add Hour column (We need Hour it for Bokeh)\n","    Table['Hours']=Table.index\n","\n","    # Convert data to bokeh data \n","    source = ColumnDataSource(Table)\n","\n","    # Create an Empty Bokeh Figure.\n","    \"\"\" first, define x_range. It should be FactorRange of str(x_axis_values) \"\"\"\n","    x_range = list(map(str, Table['Hours'].values))  \n","    x_range = FactorRange(factors=x_range)\n","\n","    \"\"\" then, create the figure \"\"\"\n","    p = figure(x_range = x_range, \n","            plot_height = plot_height,\n","            plot_width = plot_width,\n","            title='Hourly distribution of ' + plotMe.lower(),\n","            x_axis_label='Hour', \n","            y_axis_label='Frequency'\n","            )\n","    \n","    # Loop to create a barplot for each label: \n","    \"\"\" first, Define colors (one color for each label): \"\"\"\n","    if ((plotMe == 'VEHICLE TYPE CODE 1') | (plotMe == 'VEHICLE TYPE CODE 2')):\n","        colors = palettes.Category20[20] \n","    else:\n","        colors = palettes.Category20[20]\n","        colors.insert(15, '#000000')\n","\n","    \"\"\" then,\n","    Define an empty list to store legend items. \n","    The list contains tuples of label and the corresponding barplot list. \n","    Syntax:[(label, [p.vbar]), ....]   \n","    This will be used later to extract legends using Legend function.\n","    \"\"\"\n","    legend_items = []\n","\n","    \"\"\" start looping \"\"\"\n","    for i, label in enumerate(Focus):\n","        \"\"\" \n","        p.vbar is a barplot of hour vs fraction. \n","        For para see https://docs.bokeh.org/en/latest/docs/reference/plotting.html#bokeh.plotting.Figure.vbar  \n","        \"\"\"\n","        vertical_bars  = p.vbar(x='Hours',  # x_axis (column name from Table), see Table['Hours']  \n","                        top=label,          # y_axis (column name from Table), see Table \n","                        source=source,      # Table in Bokeh format \n","                        width=0.9,          # width of each bar in vbar \n","                        color=colors[i],    # color each label from the colors list\n","                        muted=True,         # Start the plot muted \n","                        muted_alpha=0.005,  # Shadow of each barplot \n","                        fill_alpha=1,       # how much to fill each bar in the barplot \n","                        line_alpha=1)       # how much to fill the border of each bar in the barplot\n","        legend_items.append((label, [vertical_bars])) # store to legend_items list\n","        \n","    # Start the interactive figure p\n","    \"\"\" First, Extract legends, legends has the label name and info from the cor. barplot's info \"\"\"\n","    legend = Legend(items=legend_items)\n","\n","    \"\"\" Then, define legends' Place. \"\"\"\n","    p.add_layout(legend, 'left')\n","\n","    \"\"\" Define the click policy \"\"\"\n","    p.legend.click_policy = 'mute'\n","\n","    \"\"\" show \"\"\"\n","    show(p)"]},{"cell_type":"markdown","metadata":{},"source":["** Bokeh-plot: Vehicle Type Code 1**"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["Bokeh_plot('VEHICLE TYPE CODE 1')"]},{"cell_type":"markdown","metadata":{},"source":["** Bokeh-plot: Vehicle Type Code 2**"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["Bokeh_plot('VEHICLE TYPE CODE 2')"]},{"cell_type":"markdown","metadata":{},"source":["** Bokeh-plot: Contributing Factor Vehicle 1**"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["Bokeh_plot('CONTRIBUTING FACTOR VEHICLE 1')"]},{"source":["** Bokeh-plot: Contributing Factor Vehicle 2 **"],"cell_type":"markdown","metadata":{}},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["Bokeh_plot('CONTRIBUTING FACTOR VEHICLE 2')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"nbformat":4,"nbformat_minor":2,"metadata":{"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":3},"orig_nbformat":2}}